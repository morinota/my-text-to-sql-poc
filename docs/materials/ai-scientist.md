
## The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery 🧑‍🔬

One of the grand challenges of artificial intelligence is developing agents capable of conducting scientific research and discovering new knowledge.
人工知能の大きな課題の一つは、科学研究を行い新しい知識を発見できるエージェントを開発することです。

While frontier models have already been used to aid human scientists—for example, for brainstorming ideas or writing code—they still require extensive manual supervision or are heavily constrained to specific tasks.
最前線のモデルはすでに人間の科学者を支援するために使用されていますが（例えば、アイデアのブレインストーミングやコードの作成など）、それでも広範な手動監視が必要であったり、特定のタスクに大きく制約されています。

We're excited to introduce The AI Scientist, the first comprehensive system for fully automatic scientific discovery, enabling Foundation Models such as Large Language Models (LLMs) to perform research independently.
私たちは、Foundation Models（大規模言語モデルなど）が独立して研究を行うことを可能にする、完全自動の科学発見のための初の包括的システムであるThe AI Scientistを紹介できることを嬉しく思います。

We provide all runs and data from our paper here, where we run each base model on each template for approximately 50 ideas.
ここでは、私たちの論文からのすべての実行とデータを提供します。ここでは、各ベースモデルを各テンプレートで約50のアイデアに対して実行します。

We highly recommend reading through some of the Claude papers to get a sense of the system's strengths and weaknesses.
システムの強みと弱みを理解するために、いくつかのClaudeの論文を読むことを強くお勧めします。

Here are some example papers generated by The AI Scientist 📝:
以下は、The AI Scientistによって生成された例の論文です📝：

- DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models
- Multi-scale Grid Noise Adaptation: Enhancing Diffusion Models For Low-dimensional Data
- GAN-Enhanced Diffusion: Boosting Sample Quality and Diversity
- DualDiff: Enhancing Mode Capture in Low-dimensional Diffusion Models via Dual-expert Denoising
- StyleFusion: Adaptive Multi-style Generation in Character-Level Language Models
- Adaptive Learning Rates for Transformers via Q-Learning
- Unlocking Grokking: A Comparative Study of Weight Initialization Strategies in Transformer Models
- Grokking Accelerated: Layer-wise Learning Rates for Transformer Generalization
- Grokking Through Compression: Unveiling Sudden Generalization via Minimal Description Length
- Accelerating Mathematical Insight: Boosting Grokking Through Strategic Data Augmentation

Note:
注意：

Caution! This codebase will execute LLM-written code.
警告！このコードベースはLLMが書いたコードを実行します。

There are various risks and challenges associated with this autonomy, including the use of potentially dangerous packages, web access, and potential spawning of processes.
この自律性には、潜在的に危険なパッケージの使用、ウェブアクセス、プロセスの生成の可能性など、さまざまなリスクと課題が伴います。

Use at your own discretion. Please make sure to containerize and restrict web access appropriately.
自己責任で使用してください。適切にコンテナ化し、ウェブアクセスを制限することを確認してください。

```
