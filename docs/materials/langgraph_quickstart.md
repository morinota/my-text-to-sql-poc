
# ğŸš€ LangGraph ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

In this tutorial, we will build a support chatbot in LangGraph that can:
ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€LangGraphã§ã‚µãƒãƒ¼ãƒˆãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ï¼š
âœ…Answer common questions by searching the web
âœ…ã‚¦ã‚§ãƒ–ã‚’æ¤œç´¢ã—ã¦ä¸€èˆ¬çš„ãªè³ªå•ã«ç­”ãˆã‚‹
âœ…Maintain conversation state across calls
âœ…å‘¼ã³å‡ºã—é–“ã§ä¼šè©±ã®çŠ¶æ…‹ã‚’ç¶­æŒã™ã‚‹
âœ…Route complex queries to a human for review
âœ…è¤‡é›‘ãªã‚¯ã‚¨ãƒªã‚’äººé–“ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã—ã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã‚‹
âœ…Use custom state to control its behavior
âœ…ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒ†ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ãã®å‹•ä½œã‚’åˆ¶å¾¡ã™ã‚‹
âœ…Rewind and explore alternative conversation paths
âœ…å·»ãæˆ»ã—ã¦ä»£æ›¿ã®ä¼šè©±ãƒ‘ã‚¹ã‚’æ¢ã‚‹

We'll start with a basic chatbot and progressively add more sophisticated capabilities, introducing key LangGraph concepts along the way.
åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‹ã‚‰å§‹ã‚ã€å¾ã€…ã«ã‚ˆã‚Šé«˜åº¦ãªæ©Ÿèƒ½ã‚’è¿½åŠ ã—ãªãŒã‚‰ã€é€”ä¸­ã§é‡è¦ãªLangGraphã®æ¦‚å¿µã‚’ç´¹ä»‹ã—ã¦ã„ãã¾ã™ã€‚

Letâ€™s dive in! ğŸŒŸ
ã•ã‚ã€å§‹ã‚ã¾ã—ã‚‡ã†ï¼ğŸŒŸ

## Setup ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

First, install the required packages and configure your environment:
ã¾ãšã€å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ç’°å¢ƒã‚’è¨­å®šã—ã¾ã™ï¼š

```
%%capture--no-stderr%pipinstall-Ulanggraphlangsmithlangchain_anthropic
```

```
importgetpassimportosdef_set_env(var:str):ifnotos.environ.get(var):os.environ[var]=getpass.getpass(f"{var}: ")_set_env("ANTHROPIC_API_KEY")
```

```
ANTHROPIC_API_KEY:  Â·Â·Â·Â·Â·Â·Â·Â·
```

Set up LangSmith for LangGraph development
LangGraphé–‹ç™ºã®ãŸã‚ã«LangSmithã‚’è¨­å®šã—ã¾ã™ã€‚
Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects.
LangSmithã«ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦ã€å•é¡Œã‚’è¿…é€Ÿã«ç‰¹å®šã—ã€LangGraphãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã¾ã—ã‚‡ã†ã€‚
LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph â€” read more about how to get started here.
LangSmithã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€LangGraphã§æ§‹ç¯‰ã—ãŸLLMã‚¢ãƒ—ãƒªã‚’ãƒ‡ãƒãƒƒã‚°ã€ãƒ†ã‚¹ãƒˆã€ç›£è¦–ã™ã‚‹ãŸã‚ã«ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã§ãã¾ã™ â€” ã“ã¡ã‚‰ã§å§‹ã‚ã‚‹æ–¹æ³•ã«ã¤ã„ã¦è©³ã—ãèª­ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚

## Part 1: Build a Basic ChatbotÂ¶ åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®æ§‹ç¯‰

We'll first create a simple chatbot using LangGraph.
ã¾ãšã€LangGraphã‚’ä½¿ç”¨ã—ã¦ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚
This chatbot will respond directly to user messages.
ã“ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯ã€ãƒ¦ãƒ¼ã‚¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ç›´æ¥å¿œç­”ã—ã¾ã™ã€‚
Though simple, it will illustrate the core concepts of building with LangGraph.
ã‚·ãƒ³ãƒ—ãƒ«ã§ã¯ã‚ã‚Šã¾ã™ãŒã€LangGraphã‚’ä½¿ç”¨ã—ãŸæ§‹ç¯‰ã®åŸºæœ¬æ¦‚å¿µã‚’ç¤ºã—ã¾ã™ã€‚
By the end of this section, you will have a built rudimentary chatbot.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ‚ã‚ã‚Šã¾ã§ã«ã¯ã€åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãŒæ§‹ç¯‰ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

Start by creating a StateGraph.
StateGraphã‚’ä½œæˆã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚
A StateGraph object defines the structure of our chatbot as a "state machine".
StateGraphã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®æ§‹é€ ã‚’ã€ŒçŠ¶æ…‹é·ç§»æ©Ÿã€ã¨ã—ã¦å®šç¾©ã—ã¾ã™ã€‚
We'll add nodes to represent the llm and functions our chatbot can call and edges to specify how the bot should transition between these functions.
ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãŒå‘¼ã³å‡ºã™ã“ã¨ãŒã§ãã‚‹llmã‚„é–¢æ•°ã‚’è¡¨ã™ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã€ã“ã‚Œã‚‰ã®é–¢æ•°é–“ã§ãƒœãƒƒãƒˆãŒã©ã®ã‚ˆã†ã«é·ç§»ã™ã‚‹ã‹ã‚’æŒ‡å®šã™ã‚‹ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ ã—ã¾ã™ã€‚

```
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)
```

Our graph can now handle two key tasks:
ã“ã‚Œã§ã€ç§ãŸã¡ã®ã‚°ãƒ©ãƒ•ã¯2ã¤ã®é‡è¦ãªã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

- Each node can receive the current State as input and output an update to the state. å„ãƒãƒ¼ãƒ‰ã¯ã€ç¾åœ¨ã®Stateã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€çŠ¶æ…‹ã®æ›´æ–°ã‚’å‡ºåŠ›ã§ãã¾ã™ã€‚
- Updates to messages will be appended to the existing list rather than overwriting it, thanks to the prebuilt add_messages function used with the Annotated syntax. Annotatedæ§‹æ–‡ã‚’ä½¿ç”¨ã—ã¦äº‹å‰ã«æ§‹ç¯‰ã•ã‚ŒãŸadd_messagesé–¢æ•°ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ›´æ–°ã¯æ—¢å­˜ã®ãƒªã‚¹ãƒˆã«è¿½åŠ ã•ã‚Œã€ä¸Šæ›¸ãã•ã‚Œã¾ã›ã‚“ã€‚

Concept
æ¦‚å¿µ
When defining a graph, the first step is to define its State.
**ã‚°ãƒ©ãƒ•ã‚’å®šç¾©ã™ã‚‹éš›ã®æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€ãã®çŠ¶æ…‹ã‚’å®šç¾©ã™ã‚‹ã“ã¨**ã§ã™ã€‚
The State includes the graph's schema and reducer functions that handle state updates.
**çŠ¶æ…‹ã«ã¯ã€ã‚°ãƒ©ãƒ•ã®ã‚¹ã‚­ãƒ¼ãƒã¨çŠ¶æ…‹æ›´æ–°ã‚’å‡¦ç†ã™ã‚‹ãƒªãƒ‡ãƒ¥ãƒ¼ã‚µé–¢æ•°**ãŒå«ã¾ã‚Œã¾ã™ã€‚
In our example, State is a TypedDict with one key: messages.
ç§ãŸã¡ã®ä¾‹ã§ã¯ã€**Stateã¯1ã¤ã®ã‚­ãƒ¼ï¼ˆmessagesï¼‰ã‚’æŒã¤TypedDict**ã§ã™ã€‚
The add_messages reducer function is used to append new messages to the list instead of overwriting it.
add_messagesãƒªãƒ‡ãƒ¥ãƒ¼ã‚µé–¢æ•°ã¯ã€æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã€ä¸Šæ›¸ãã™ã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
Keys without a reducer annotation will overwrite previous values.
**ãƒªãƒ‡ãƒ¥ãƒ¼ã‚µæ³¨é‡ˆã®ãªã„ã‚­ãƒ¼ã¯ã€ä»¥å‰ã®å€¤ã‚’ä¸Šæ›¸ãã—ã¾ã™**ã€‚
Learn more about state, reducers, and related concepts in this guide.
ã“ã®ã‚¬ã‚¤ãƒ‰ã§çŠ¶æ…‹ã€ãƒªãƒ‡ãƒ¥ãƒ¼ã‚µã€ãŠã‚ˆã³é–¢é€£ã™ã‚‹æ¦‚å¿µã«ã¤ã„ã¦è©³ã—ãå­¦ã³ã¾ã—ã‚‡ã†ã€‚

Next, add a "chatbot" node.
æ¬¡ã«ã€ã€Œãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¾ã™ã€‚
Nodes represent units of work.
**ãƒãƒ¼ãƒ‰ã¯ä½œæ¥­ã®å˜ä½ã‚’è¡¨ã—ã¾ã™**ã€‚
They are typically regular python functions.
**ãƒãƒ¼ãƒ‰ã¯é€šå¸¸ã€é€šå¸¸ã®Pythoné–¢æ•°**ã§ã™ã€‚

```
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")

def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}
# The first argument is the unique node name
# The second argument is the function or object that will be called whenever
# the node is used.
graph_builder.add_node("chatbot", chatbot)
```

Notice how the chatbot node function takes the current State as input and returns a dictionary containing an updated messages list under the key "messages".
chatbotãƒãƒ¼ãƒ‰é–¢æ•°ãŒ**ç¾åœ¨ã®Stateã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Š**ã€**"messages"ã‚­ãƒ¼ã®ä¸‹ã«æ›´æ–°ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’å«ã‚€è¾æ›¸ã‚’è¿”ã™ã“ã¨**ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚
This is the basic pattern for all LangGraph node functions.
ã“ã‚Œã¯ã€**ã™ã¹ã¦ã®LangGraphãƒãƒ¼ãƒ‰é–¢æ•°ã®åŸºæœ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³**ã§ã™ã€‚

The add_messages function in our State will append the llm's response messages to whatever messages are already in the state.
ç§ãŸã¡ã®Stateå†…ã®add_messagesé–¢æ•°ã¯ã€llmã®å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çŠ¶æ…‹ã«æ—¢ã«å­˜åœ¨ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿½åŠ ã—ã¾ã™ã€‚

Next, add an entry point.
æ¬¡ã«ã€**ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’è¿½åŠ **ã—ã¾ã™ã€‚
This tells our graph where to start its work each time we run it.
ã“ã‚Œã¯ã€ã‚°ãƒ©ãƒ•ãŒå®Ÿè¡Œã•ã‚Œã‚‹ãŸã³ã«ä½œæ¥­ã‚’é–‹å§‹ã™ã‚‹å ´æ‰€ã‚’æŒ‡ç¤ºã—ã¾ã™ã€‚(å ´æ‰€ã£ã¦ãƒãƒ¼ãƒ‰ã£ã¦ã“ã¨ã ã‚ˆã­...!:thinking_face:)

```
graph_builder.add_edge(START, "chatbot")
```

Similarly, set a finish point.
åŒæ§˜ã«ã€ãƒ•ã‚£ãƒ‹ãƒƒã‚·ãƒ¥ãƒã‚¤ãƒ³ãƒˆã‚’è¨­å®šã—ã¾ã™ã€‚
This instructs the graph "any time this node is run, you can exit."
ã“ã‚Œã¯ã€ã‚°ãƒ©ãƒ•ã«ã€Œ**ã“ã®ãƒãƒ¼ãƒ‰ãŒå®Ÿè¡Œã•ã‚Œã‚‹ãŸã³ã«ã€çµ‚äº†ã§ãã¾ã™**ã€ã¨æŒ‡ç¤ºã—ã¾ã™ã€‚

```
graph_builder.add_edge("chatbot", END)
```

Finally, we'll want to be able to run our graph.
æœ€å¾Œã«ã€**ã‚°ãƒ©ãƒ•ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ãŸã„**ã¨æ€ã„ã¾ã™ã€‚
To do so, call "compile()" on the graph builder.
ãã®ãŸã‚ã«ã¯ã€ã‚°ãƒ©ãƒ•ãƒ“ãƒ«ãƒ€ãƒ¼ã§"compile()"ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚
This creates a "CompiledGraph" we can use invoke on our state.
ã“ã‚Œã«ã‚ˆã‚Šã€çŠ¶æ…‹ã§**invokeã‚’ä½¿ç”¨ã§ãã‚‹ã€ŒCompiledGraphã€ãŒä½œæˆã•ã‚Œã¾ã™**ã€‚

```
graph = graph_builder.compile()
```

You can visualize the graph using the get_graph method and one of the "draw" methods, like draw_ascii or draw_png.
get_graphãƒ¡ã‚½ãƒƒãƒ‰ã¨draw_asciiã‚„draw_pngã®ã„ãšã‚Œã‹ã®ã€Œdrawã€ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ã‚°ãƒ©ãƒ•ã‚’è¦–è¦šåŒ–ã§ãã¾ã™ã€‚
The draw methods each require additional dependencies.
drawãƒ¡ã‚½ãƒƒãƒ‰ã¯ãã‚Œãã‚Œè¿½åŠ ã®ä¾å­˜é–¢ä¿‚ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚

```
from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass
```

Now let's run the chatbot!
ã•ã‚ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼
Tip: You can exit the chat loop at any time by typing "quit", "exit", or "q".
ãƒ’ãƒ³ãƒˆï¼šã„ã¤ã§ã‚‚ã€Œquitã€ã€ã€Œexitã€ã€ã¾ãŸã¯ã€Œqã€ã¨å…¥åŠ›ã™ã‚‹ã“ã¨ã§ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†ã§ãã¾ã™ã€‚

```
def stream_graph_updates(user_input: str):
    for event in graph.stream({"messages": [("user", user_input)]}):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)
    while True:
        try:
            user_input = input("User: ")
            if user_input.lower() in ["quit", "exit", "q"]:
                print("Goodbye!")
                break
            stream_graph_updates(user_input)
        except:
            # fallback if input() is not available
            user_input = "What do you know about LangGraph?"
            print("User: " + user_input)
            stream_graph_updates(user_input)
            break
```

```
Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. 
ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼šLangGraphã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦çŠ¶æ…‹ã‚’æŒã¤ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã®ã«å½¹ç«‹ã¤ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ 
It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. 
ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚„çŠ¶æ…‹é·ç§»æ©Ÿã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã‚’æä¾›ã—ã€è¤‡æ•°ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚„è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç›¸äº’ä½œç”¨ã‚’èª¿æ•´ã—ã¾ã™ã€‚ 
LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. 
LangGraphã¯LangChainã®ä¸Šã«æ§‹ç¯‰ã•ã‚Œã¦ãŠã‚Šã€ãã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æ´»ç”¨ã—ãªãŒã‚‰ã€ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®èª¿æ•´æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ã„ã¾ã™ã€‚ 
It's particularly useful for developing more complex, stateful AI applications that go beyond simple query-response interactions. 
ã“ã‚Œã¯ã€å˜ç´”ãªã‚¯ã‚¨ãƒªå¿œç­”ã®ç›¸äº’ä½œç”¨ã‚’è¶…ãˆãŸã€ã‚ˆã‚Šè¤‡é›‘ã§çŠ¶æ…‹ã‚’æŒã¤AIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®é–‹ç™ºã«ç‰¹ã«å½¹ç«‹ã¡ã¾ã™ã€‚ 
Goodbye! 
ã•ã‚ˆã†ãªã‚‰ï¼ 
```

Congratulations! You've built your first chatbot using LangGraph. This bot can engage in basic conversation by taking user input and generating responses using an LLM. You can inspect a LangSmith Trace for the call above at the provided link.
ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼LangGraphã‚’ä½¿ç”¨ã—ã¦æœ€åˆã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚ã“ã®ãƒœãƒƒãƒˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å—ã‘å–ã‚Šã€LLMã‚’ä½¿ç”¨ã—ã¦å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§åŸºæœ¬çš„ãªä¼šè©±ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚ä¸Šè¨˜ã®å‘¼ã³å‡ºã—ã«å¯¾ã—ã¦LangSmithãƒˆãƒ¬ãƒ¼ã‚¹ã‚’æ¤œæŸ»ã§ãã¾ã™ã€‚

However, you may have noticed that the bot's knowledge is limited to what's in its training data.
ãŸã ã—ã€ãƒœãƒƒãƒˆã®çŸ¥è­˜ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‚‚ã®ã«é™ã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ã«æ°—ä»˜ã„ãŸã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
In the next part, we'll add a web search tool to expand the bot's knowledge and make it more capable.
æ¬¡ã®éƒ¨åˆ†ã§ã¯ã€ãƒœãƒƒãƒˆã®çŸ¥è­˜ã‚’æ‹¡å¼µã—ã€ã‚ˆã‚Šèƒ½åŠ›ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã‚¦ã‚§ãƒ–æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’è¿½åŠ ã—ã¾ã™ã€‚

Below is the full code for this section for your reference:
ä»¥ä¸‹ã¯ã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ã§ã™ã€‚

```
from typing import Annotated
from langchain_anthropic import ChatAnthropic
from typing_extensions import TypedDict
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)
llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")

def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}

# The first argument is the unique node name

# The second argument is the function or object that will be called whenever

# the node is used

graph_builder.add_node("chatbot", chatbot)
graph_builder.set_entry_point("chatbot")
graph_builder.set_finish_point("chatbot")
graph = graph_builder.compile()
```

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## Part 2: ğŸ› ï¸ Enhancing the Chatbot with Tools

To handle queries our chatbot can't answer "from memory", we'll integrate a web search tool.
ç§ãŸã¡ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãŒã€Œè¨˜æ†¶ã€ã‹ã‚‰ç­”ãˆã‚‰ã‚Œãªã„ã‚¯ã‚¨ãƒªã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€ã‚¦ã‚§ãƒ–æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’çµ±åˆã—ã¾ã™ã€‚

Our bot can use this tool to find relevant information and provide better responses.
ç§ãŸã¡ã®ãƒœãƒƒãƒˆã¯ã€ã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦é–¢é€£æƒ…å ±ã‚’è¦‹ã¤ã‘ã€ã‚ˆã‚Šè‰¯ã„å¿œç­”ã‚’æä¾›ã§ãã¾ã™ã€‚

Before we start, make sure you have the necessary packages installed and API keys set up:
å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã€APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

First, install the requirements to use the Tavily Search Engine, and set your TAVILY_API_KEY.
ã¾ãšã€Tavily Search Engineã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã®è¦ä»¶ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€TAVILY_API_KEYã‚’è¨­å®šã—ã¾ã™ã€‚

```
%%capture --no-stderr %pip install -U tavily-python langchain_community
```

```
_set_env("TAVILY_API_KEY")
```

```
TAVILY_API_KEY:  Â·Â·Â·Â·Â·Â·Â·Â·
```

```
from langchain_community.tools.tavily_search import TavilySearchResults
tool = TavilySearchResults(max_results=2)
tools = [tool]
tool.invoke("What's a 'node' in LangGraph?")
```

```
[{'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141','content': 'Nodes: Nodes are the building blocks of your LangGraph. Each node represents a function or a computation step. You define nodes to perform specific tasks, such as processing input, making ...'},{'url': 'https://saksheepatil05.medium.com/demystifying-langgraph-a-beginner-friendly-dive-into-langgraph-concepts-5ffe890ddac0','content': 'Nodes (Tasks): Nodes are like the workstations on the assembly line. Each node performs a specific task on the product. In LangGraph, nodes are Python functions that take the current state, do some work, and return an updated state. Next, we define the nodes, each representing a task in our sandwich-making process.'}]
```

The results are page summaries our chat bot can use to answer questions.
çµæœã¯ã€ç§ãŸã¡ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãŒè³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã‚‹**ãƒšãƒ¼ã‚¸ã®è¦ç´„**ã§ã™ã€‚

Next, we'll start defining our graph.
æ¬¡ã«ã€ã‚°ãƒ©ãƒ•ã®å®šç¾©ã‚’å§‹ã‚ã¾ã™ã€‚

The following is all the same as in Part 1, except we have added bind_tools on our LLM.
ä»¥ä¸‹ã¯ã€Part 1ã¨åŒã˜ã§ã™ãŒã€LLMã«bind_toolsã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚

This lets the LLM know the correct JSON format to use if it wants to use our search engine.
ã“ã‚Œã«ã‚ˆã‚Šã€LLMã¯æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã«æ­£ã—ã„JSONå½¢å¼ã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```
from typing import Annotated
from langchain_anthropic import ChatAnthropic
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)
llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")

# Modification: tell the LLM which tools it can call

llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
```

Next we need to create a function to actually run the tools if they are called.
æ¬¡ã«ã€**ãƒ„ãƒ¼ãƒ«ãŒå‘¼ã³å‡ºã•ã‚ŒãŸå ´åˆã«å®Ÿéš›ã«ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°**ã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
(chatbotã¨ã¯åˆ¥ã®ã€ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ãƒãƒ¼ãƒ‰ã‚’å®šç¾©ã™ã‚‹æ„Ÿã˜...!:thinking_face:)

We'll do this by adding the tools to a new node.
ã“ã‚Œã‚’æ–°ã—ã„ãƒãƒ¼ãƒ‰ã«ãƒ„ãƒ¼ãƒ«ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§è¡Œã„ã¾ã™ã€‚

Below, we implement a BasicToolNode that checks the most recent message in the state and calls tools if the message contains tool_calls.
ä»¥ä¸‹ã«ã€**çŠ¶æ…‹å†…ã®æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã« tool_calls ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã«ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™BasicToolNodeã‚’å®Ÿè£…**ã—ã¾ã™ã€‚

It relies on the LLM's tool_calling support, which is available in Anthropic, OpenAI, Google Gemini, and a number of other LLM providers.
ã“ã‚Œã¯ã€Anthropicã€OpenAIã€Google Geminiã€ãŠã‚ˆã³ä»–ã®å¤šãã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§åˆ©ç”¨å¯èƒ½ãª**LLMã® tool_calling ã‚µãƒãƒ¼ãƒˆã«ä¾å­˜**ã—ã¦ã„ã¾ã™ã€‚

We will later replace this with LangGraph's prebuilt ToolNode to speed things up, but building it ourselves first is instructive.
**å¾Œã§ã“ã‚Œã‚’LangGraphã®ãƒ—ãƒªãƒ“ãƒ«ãƒ‰ToolNodeã«ç½®ãæ›ãˆã¦é€Ÿåº¦ã‚’ä¸Šã’ã¾ã™**ãŒã€æœ€åˆã«è‡ªåˆ†ãŸã¡ã§æ§‹ç¯‰ã™ã‚‹ã“ã¨ã¯æ•™è‚²çš„ã§ã™ã€‚

```
import json
from langchain_core.messages import ToolMessage

class BasicToolNode:
    """A node that runs the tools requested in the last AIMessage."""

    def __init__(self, tools: list) -> None:
        self.tools_by_name = {tool.name: tool for tool in tools}

    def __call__(self, inputs: dict):
        if messages := inputs.get("messages", []):
            message = messages[-1]
        else:
            raise ValueError("No message found in input")
        
        outputs = []
        for tool_call in message.tool_calls:
            tool_result = self.tools_by_name[tool_call["name"]].invoke(tool_call["args"])
            outputs.append(ToolMessage(content=json.dumps(tool_result), name=tool_call["name"], tool_call_id=tool_call["id"],))
        
        return {"messages": outputs}

tool_node = BasicToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)
```

With the tool node added, we can define the conditional_edges.
ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰ãŒè¿½åŠ ã•ã‚ŒãŸã®ã§ã€ conditional_edges ã‚’å®šç¾©ã§ãã¾ã™ã€‚

Recall that edges route the control flow from one node to the next.
**ã‚¨ãƒƒã‚¸ã¯ã€1ã¤ã®ãƒãƒ¼ãƒ‰ã‹ã‚‰æ¬¡ã®ãƒãƒ¼ãƒ‰ã¸ã®åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼ã‚’ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã—ã¾ã™**ã€‚(routingé–¢æ•°ã‚’å®šç¾©ã™ã‚‹!)
Conditional edges usually contain "if" statements to route to different nodes depending on the current graph state.
**æ¡ä»¶ä»˜ãã‚¨ãƒƒã‚¸ã¯é€šå¸¸ã€ã€Œifã€æ–‡ã‚’å«ã¿ã€ç¾åœ¨ã®ã‚°ãƒ©ãƒ•çŠ¶æ…‹ã«å¿œã˜ã¦ç•°ãªã‚‹ãƒãƒ¼ãƒ‰ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**ã—ã¾ã™ã€‚
These functions receive the current graph state and return a string or list of strings indicating which node(s) to call next.
ã“ã‚Œã‚‰ã®é–¢æ•°ã¯ã€**ç¾åœ¨ã®ã‚°ãƒ©ãƒ•çŠ¶æ…‹ã‚’å—ã‘å–ã‚Šã€æ¬¡ã«å‘¼ã³å‡ºã™ãƒãƒ¼ãƒ‰ã‚’ç¤ºã™æ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™**ã€‚

Below, we define a router function called route_tools, that checks for tool_calls in the chatbot's output.
ä»¥ä¸‹ã«ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å‡ºåŠ›ã«tool_callsãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹**route_toolsã¨ã„ã†ãƒ«ãƒ¼ã‚¿é–¢æ•°ã‚’å®šç¾©**ã—ã¾ã™ã€‚

Provide this function to the graph by calling add_conditional_edges, which tells the graph that whenever the chatbot node completes to check this function to see where to go next.
**ã“ã®é–¢æ•°ã‚’add_conditional_edgesã‚’å‘¼ã³å‡ºã™ã“ã¨ã§ã‚°ãƒ©ãƒ•ã«æä¾›**ã—ã¾ã™ã€‚**ã“ã‚Œã«ã‚ˆã‚Šã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒãƒ¼ãƒ‰ãŒå®Œäº†ã™ã‚‹ãŸã³ã«ã€ã“ã®é–¢æ•°ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æ¬¡ã«ã©ã“ã«è¡Œãã‹ã‚’ç¢ºèª**ã—ã¾ã™ã€‚

The condition will route to tools if tool calls are present and END if not.
æ¡ä»¶ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãƒ„ãƒ¼ãƒ«ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã—ã€å­˜åœ¨ã—ãªã„å ´åˆã¯ENDã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã—ã¾ã™ã€‚

Later, we will replace this with the prebuilt tools_condition to be more concise, but implementing it ourselves first makes things more clear.
å¾Œã§ã“ã‚Œã‚’ãƒ—ãƒªãƒ“ãƒ«ãƒ‰ã®tools_conditionã«ç½®ãæ›ãˆã¦ã‚ˆã‚Šç°¡æ½”ã«ã—ã¾ã™ãŒã€æœ€åˆã«è‡ªåˆ†ãŸã¡ã§å®Ÿè£…ã™ã‚‹ã“ã¨ã§ã‚ˆã‚Šæ˜ç¢ºã«ãªã‚Šã¾ã™ã€‚

```

from typing import Literal

def route_tools(state: State):
    """Use in the conditional_edge to route to the ToolNode if the last message has tool calls. Otherwise, route to the end."""

    if isinstance(state, list):
        ai_message = state[-1]
    elif messages := state.get("messages", []):
        ai_message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")
    
    if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0:
        return "tools"
    
    return END

# The `tools_condition` function returns "tools" if the chatbot asks to use a tool, and "END" if it is fine directly responding

# This conditional routing defines the main agent loop

graph_builder.add_conditional_edges("chatbot", route_tools,
    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node
    # It defaults to the identity function, but if you want to use a node named something else apart from "tools",
    # You can update the value of the dictionary to something else
    # e.g., "tools": "my_tools"
    {"tools": "tools", END: END},
)

# Any time a tool is called, we return to the chatbot to decide the next step

graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
graph = graph_builder.compile()
```

Notice that conditional edges start from a single node.
**æ¡ä»¶ä»˜ãã‚¨ãƒƒã‚¸ã¯å˜ä¸€ã®ãƒãƒ¼ãƒ‰ã‹ã‚‰å§‹ã¾ã‚‹**ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

This tells the graph "any time the 'chatbot' node runs, either go to 'tools' if it calls a tool, or end the loop if it responds directly.
**ã“ã‚Œã¯ã€ã‚°ãƒ©ãƒ•ã«ã€Œãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒãƒ¼ãƒ‰ãŒå®Ÿè¡Œã•ã‚Œã‚‹ãŸã³ã«ã€ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™å ´åˆã¯'tools'ã«è¡Œãã€ç›´æ¥å¿œç­”ã™ã‚‹å ´åˆã¯ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†ã™ã‚‹ã€ã¨ä¼ãˆã¾ã™**ã€‚

Like the prebuilt tools_condition, our function returns the END string if no tool calls are made.
ãƒ—ãƒªãƒ“ãƒ«ãƒ‰ã® tools_condition ã®ã‚ˆã†ã«ã€ç§ãŸã¡ã®é–¢æ•°ã¯**ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒè¡Œã‚ã‚Œãªã„å ´åˆã«ENDæ–‡å­—åˆ—ã‚’è¿”ã—ã¾ã™**ã€‚
When the graph transitions to END, it has no more tasks to complete and ceases execution.
**ã‚°ãƒ©ãƒ•ãŒENDã«é·ç§»ã™ã‚‹ã¨ã€å®Œäº†ã™ã¹ãã‚¿ã‚¹ã‚¯ãŒãªããªã‚Šã€å®Ÿè¡Œã‚’åœæ­¢ã™ã‚‹**
Because the condition can return END, we don't need to explicitly set a finish_point this time.
æ¡ä»¶ãŒENDã‚’è¿”ã™ã“ã¨ãŒã§ãã‚‹ãŸã‚ã€ä»Šå›ã¯æ˜ç¤ºçš„ã«finish_pointã‚’è¨­å®šã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
Our graph already has a way to finish!
ç§ãŸã¡ã®ã‚°ãƒ©ãƒ•ã«ã¯ã™ã§ã«çµ‚äº†ã™ã‚‹æ–¹æ³•ãŒã‚ã‚Šã¾ã™! (chatbotãƒãƒ¼ãƒ‰ã‹ã‚‰ENDãƒãƒ¼ãƒ‰ã¸ã®æ¡ä»¶ä»˜ãã‚¨ãƒƒã‚¸ãŒã™ã§ã«å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ã‚‰ã­...!:thinking_face:)

Let's visualize the graph we've built.
ç§ãŸã¡ãŒæ§‹ç¯‰ã—ãŸã‚°ãƒ©ãƒ•ã‚’è¦–è¦šåŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
The following function has some additional dependencies to run that are unimportant for this tutorial.
ä»¥ä¸‹ã®é–¢æ•°ã«ã¯ã€ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã«ã¯é‡è¦ã§ãªã„è¿½åŠ ã®ä¾å­˜é–¢ä¿‚ãŒã‚ã‚Šã¾ã™ã€‚

```
from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass
```

Now we can ask the bot questions outside its training data.
ã“ã‚Œã§ã€ãƒœãƒƒãƒˆã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å¤–ã®è³ªå•ã‚’ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```

while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break
        stream_graph_updates(user_input)
    except:
        # fallback if input() is not available
        user_input = "What do you know about LangGraph?"
        print("User: " + user_input)
        stream_graph_updates(user_input)
        break

```

```
Assistant: [{'text': "To provide you with accurate and up-to-date information about LangGraph, I'll need to search for the latest details. Let me do that for you.", 'type': 'text'}, {'id': 'toolu_01Q588CszHaSvvP2MxRq9zRD', 'input': {'query': 'LangGraph AI tool information'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Assistant: [{"url": "https://www.langchain.com/langgraph", "content": "LangGraph sets the foundation for how we can build and scale AI workloads â€” from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution ..."}, {"url": "https://github.com/langchain-ai/langgraph", "content": "Overview. LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures ..."}]
Assistant: Based on the search results, I can provide you with information about LangGraph: 1. Purpose: LangGraph is a library designed for building stateful, multi-actor applications with Large Language Models (LLMs). It's particularly useful for creating agent and multi-agent workflows. 2. Developer: LangGraph is developed by LangChain, a company known for its tools and frameworks in the AI and LLM space. 3. Key Features: - Cycles: LangGraph allows the definition of flows that involve cycles, which is essential for most agentic architectures. - Controllability: It offers enhanced control over the application flow. - Persistence: The library provides ways to maintain state and persistence in LLM-based applications. 4. Use Cases: LangGraph can be used for various applications, including: - Conversational agents - Complex task automation - Custom LLM-backed experiences 5. Integration: LangGraph works in conjunction with LangSmith, another tool by LangChain, to provide an out-of-the-box solution for building complex, production-ready features with LLMs. 6. Significance: LangGraph is described as setting the foundation for building and scaling AI workloads. It's positioned as a key tool in the next chapter of LLM-based application development, particularly in the realm of agentic AI. 7. Availability: LangGraph is open-source and available on GitHub, which suggests that developers can access and contribute to its codebase. 8. Comparison to Other Frameworks: LangGraph is noted to offer unique benefits compared to other LLM frameworks, particularly in its ability to handle cycles, provide controllability, and maintain persistence. LangGraph appears to be a significant tool in the evolving landscape of LLM-based application development, offering developers new ways to create more complex, stateful, and interactive AI systems. Goodbye!
```

Our chatbot still can't remember past interactions on its own, limiting its ability to have coherent, multi-turn conversations.
ç§ãŸã¡ã®**ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯ã¾ã éå»ã®ã‚„ã‚Šå–ã‚Šã‚’è‡ªåˆ†ã§è¨˜æ†¶ã™ã‚‹ã“ã¨ãŒã§ããš**ã€ä¸€è²«ã—ãŸãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®ä¼šè©±ã‚’æŒã¤èƒ½åŠ›ãŒåˆ¶é™ã•ã‚Œã¦ã„ã¾ã™ã€‚
In the next part, we'll add memory to address this.
æ¬¡ã®éƒ¨åˆ†ã§ã¯ã€**ã“ã‚Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ãƒ¡ãƒ¢ãƒªã‚’è¿½åŠ **ã—ã¾ã™ã€‚

The full code for the graph we've created in this section is reproduced below, replacing our BasicToolNode for the prebuilt ToolNode, and our route_tools condition with the prebuilt tools_condition.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ä½œæˆã—ãŸã‚°ãƒ©ãƒ•ã®å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã«å†ç¾ã•ã‚Œã¦ãŠã‚Šã€**BasicToolNodeã‚’ãƒ—ãƒªãƒ“ãƒ«ãƒ‰ã®ToolNodeã«ç½®ãæ›ãˆã€route_toolsæ¡ä»¶é–¢æ•°ã‚’ãƒ—ãƒªãƒ“ãƒ«ãƒ‰ã®tools_conditionã«ç½®ãæ›ãˆã¦ã„ã¾ã™**ã€‚

```python
from typing import Annotated
from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)
tool = TavilySearchResults(max_results=2)
tools = [tool]
llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)
graph_builder.add_conditional_edges("chatbot", tools_condition,)

# Any time a tool is called, we return to the chatbot to decide the next step

graph_builder.add_edge("tools", "chatbot")
graph_builder.set_entry_point("chatbot")
graph = graph_builder.compile()
```

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## Part 3: Adding Memory to the ChatbotÂ¶ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã«ãƒ¡ãƒ¢ãƒªã‚’è¿½åŠ ã™ã‚‹

Our chatbot can now use tools to answer user questions, but it doesn't remember the context of previous interactions.
ç§ãŸã¡ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸãŒã€ä»¥å‰ã®ã‚„ã‚Šå–ã‚Šã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨˜æ†¶ã—ã¦ã„ã¾ã›ã‚“ã€‚
This limits its ability to have coherent, multi-turn conversations.
ã“ã‚Œã«ã‚ˆã‚Šã€ä¸€è²«ã—ãŸãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®ä¼šè©±ã‚’è¡Œã†èƒ½åŠ›ãŒåˆ¶é™ã•ã‚Œã¾ã™ã€‚
LangGraph solves this problem through persistent checkpointing.
LangGraphã¯ã€**æ°¸ç¶šçš„ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**ã‚’é€šã˜ã¦ã“ã®å•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚
If you provide a checkpointer when compiling the graph and a thread_id when calling your graph, LangGraph automatically saves the state after each step.
**ã‚°ãƒ©ãƒ•ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹éš›ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ã‚¿ã‚’æä¾›ã—ã€ã‚°ãƒ©ãƒ•ã‚’å‘¼ã³å‡ºã™éš›ã«thread_idã‚’æŒ‡å®šã™ã‚‹**ã¨ã€LangGraphã¯å„ã‚¹ãƒ†ãƒƒãƒ—ã®å¾Œã«è‡ªå‹•çš„ã«çŠ¶æ…‹ã‚’ä¿å­˜ã—ã¾ã™ã€‚
When you invoke the graph again using the same thread_id, the graph loads its saved state, allowing the chatbot to pick up where it left off.
åŒã˜thread_idã‚’ä½¿ç”¨ã—ã¦ã‚°ãƒ©ãƒ•ã‚’å†åº¦å‘¼ã³å‡ºã™ã¨ã€**ã‚°ãƒ©ãƒ•ã¯ä¿å­˜ã•ã‚ŒãŸçŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯å‰å›ã®ç¶šãã‹ã‚‰å†é–‹ã§ãã‚‹**ã€‚
We will see later that checkpointing is much more powerful than simple chat memory - it lets you save and resume complex state at any time for error recovery, human-in-the-loop workflows, time travel interactions, and more.
å¾Œã§è¦‹ã¦ã„ãã¨ã€**ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯å˜ç´”ãªãƒãƒ£ãƒƒãƒˆãƒ¡ãƒ¢ãƒªã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«å¼·åŠ›ã§ã‚ã‚Šã€ã‚¨ãƒ©ãƒ¼å›å¾©ã‚„äººé–“ã®ä»‹å…¥ãŒå¿…è¦ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒ™ãƒ«ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãªã©ã®ãŸã‚ã«ã€ã„ã¤ã§ã‚‚è¤‡é›‘ãªçŠ¶æ…‹ã‚’ä¿å­˜ã—ã¦å†é–‹ã§ãã‚‹**ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
But before we get too ahead of ourselves, let's add checkpointing to enable multi-turn conversations.
ã—ã‹ã—ã€å…ˆã«é€²ã‚€å‰ã«ã€ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®ä¼šè©±ã‚’å¯èƒ½ã«ã™ã‚‹ãŸã‚ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’è¿½åŠ ã—ã¾ã—ã‚‡ã†ã€‚

To get started, create a MemorySaver checkpointer.
ã¾ãšã€MemorySaverãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ã‚¿ã‚’ä½œæˆã—ã¾ã™ã€‚

```
from langgraph.checkpoint.memory import MemorySaver
memory = MemorySaver()
```

Notice we're using an in-memory checkpointer.
**ç§ãŸã¡ã¯ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹**ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚
This is convenient for our tutorial (it saves it all in-memory).
ã“ã‚Œã¯ç§ãŸã¡ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã«ã¨ã£ã¦ä¾¿åˆ©ã§ã™ï¼ˆ**ã™ã¹ã¦ã‚’ãƒ¡ãƒ¢ãƒªå†…ã«ä¿å­˜**ã—ã¾ã™ï¼‰ã€‚
In a production application, you would likely change this to use SqliteSaver or PostgresSaver and connect to your own DB.
æœ¬ç•ªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ã“ã‚Œã‚’SqliteSaverã¾ãŸã¯PostgresSaverã‚’ä½¿ç”¨ã—ã¦è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶šã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ã™ã‚‹ã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚

Next define the graph.
æ¬¡ã«ã€ã‚°ãƒ©ãƒ•ã‚’å®šç¾©ã—ã¾ã™ã€‚
Now that you've already built your own BasicToolNode, we'll replace it with LangGraph's prebuilt ToolNode and tools_condition, since these do some nice things like parallel API execution.
ã™ã§ã«**ç‹¬è‡ªã®BasicToolNodeã‚’æ§‹ç¯‰ã—ã¦ã„ã‚‹ã®ã§ã€ã“ã‚Œã‚’LangGraphã®äº‹å‰æ§‹ç¯‰ã•ã‚ŒãŸToolNodeã¨tools_conditionã«ç½®ãæ›ãˆã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä¸¦åˆ—APIå®Ÿè¡Œãªã©ã®ä¾¿åˆ©ãªæ©Ÿèƒ½ãŒæä¾›**ã•ã‚Œã¾ã™ã€‚(ã‚ã€ã‚„ã£ã±ã‚Šprebuildã®é–¢æ•°ã®æ–¹ãŒã„ã„ã‚“ã ãª:thinking_face:)
Apart from that, the following is all copied from Part 2.
ãã‚Œä»¥å¤–ã¯ã€ä»¥ä¸‹ã®å†…å®¹ã¯ã™ã¹ã¦Part 2ã‹ã‚‰ã‚³ãƒ”ãƒ¼ã—ãŸã‚‚ã®ã§ã™ã€‚

```python
from typing import Annotated
from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)
tool = TavilySearchResults(max_results=2)
tools = [tool]
llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)
graph_builder.add_conditional_edges("chatbot", tools_condition,)

# Any time a tool is called, we return to the chatbot to decide the next step

graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
```

Finally, compile the graph with the provided checkpointer.
æœ€å¾Œã«ã€æä¾›ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã‚°ãƒ©ãƒ•ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚

```
graph = graph_builder.compile(checkpointer=memory)
```

Notice the connectivity of the graph hasn't changed since Part 2.
**ã‚°ãƒ©ãƒ•ã®æ¥ç¶šæ€§ã¯Part 2ã‹ã‚‰å¤‰ã‚ã£ã¦ã„ãªã„ã“ã¨ã«æ³¨æ„**ã—ã¦ãã ã•ã„ã€‚
All we are doing is checkpointing the State as the graph works through each node.
ç§ãŸã¡ãŒè¡Œã£ã¦ã„ã‚‹ã®ã¯ã€**ã‚°ãƒ©ãƒ•ãŒå„ãƒãƒ¼ãƒ‰ã‚’é€šéã™ã‚‹éš›ã«Stateã‚’ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã™ã‚‹ã“ã¨ã ã‘**ã§ã™ã€‚(å„ãƒãƒ¼ãƒ‰ã®å‡¦ç†ãŒã‚ˆã°ã‚Œã‚‹ãŸã³ã«ã€ãã®æ™‚ç‚¹ã®Stateã‚’ãã‚Œãã‚Œä¿å­˜ã—ã¦ãŠãã£ã¦ã“ã¨ã£ã½ã„...!! å†åˆ©ç”¨ã‚„ã‚¨ãƒ©ãƒ¼æ™‚ã®å†é–‹ã«ä¾¿åˆ©ã£ã¦ã“ã¨??:thinking_face:)

```python
from IPython.display import Image, display
try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass
```

Now you can interact with your bot!
ã“ã‚Œã§ã€ãƒœãƒƒãƒˆã¨å¯¾è©±ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼
First, pick a thread to use as the key for this conversation.
ã¾ãšã€ã“ã®ä¼šè©±ã®ã‚­ãƒ¼ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é¸æŠã—ã¾ã™ã€‚

```

config = {"configurable": {"thread_id": "1"}}

```

Next, call your chat bot.
æ¬¡ã«ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚

```python
user_input = "Hi there! My name is Will."

# The config is the **second positional argument** to stream() or invoke()

events = graph.stream({"messages": [("user", user_input)]}, config, stream_mode="values")
for event in events:
    event["messages"][-1].pretty_print()
```

```shell
================================[1m Human Message [0m=================================
Hi there! My name is Will.
==================================[1m Ai Message [0m==================================
Hello Will! It's nice to meet you. How can I assist you today? Is there anything specific you'd like to know or discuss?
```

Let's ask a followup: see if it remembers your name.
æ¬¡ã«ã€ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã‚’å°‹ã­ã¦ã¿ã¾ã—ã‚‡ã†ï¼šã‚ãªãŸã®åå‰ã‚’è¦šãˆã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚
(ç¶šã‘ã¦ä¸Šã®ã‚³ãƒ¼ãƒ‰ã®ä¸‹ã«æ›¸ã! ãƒ¡ãƒ¢ãƒªä¸Šã«checkpointingã—ã¦ã‚‹ã®ã§ã€ä¸€å›ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è½ã¨ã™ã¨ãƒ¡ãƒ¢ãƒªãŒè§£æ”¾ã•ã‚Œã¦ã—ã¾ã†)

```

user_input = "Remember my name?"

# The config is the **second positional argument** to stream() or invoke()

events = graph.stream({"messages": [("user", user_input)]}, config, stream_mode="values")
for event in events:
    event["messages"][-1].pretty_print()
```

```

================================[1m Human Message [0m=================================
Remember my name?
==================================[1m Ai Message [0m==================================
Of course, I remember your name, Will. I always try to pay attention to important details that users share with me. Is there anything else you'd like to talk about or any questions you have? I'm here to help with a wide range of topics or tasks.

```

Don't believe me? Try this using a different config.
ä¿¡ã˜ã‚‰ã‚Œãªã„ã§ã™ã‹ï¼Ÿç•°ãªã‚‹è¨­å®šã‚’ä½¿ç”¨ã—ã¦ã“ã‚Œã‚’è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚
(æŒ‡å®šã™ã‚‹thread_idã‚’å¤‰ãˆã‚‹ã¨ã€å…ƒã®thread_idã®ãƒ¡ãƒ¢ãƒªã¯èª­ã¿è¾¼ã¾ã‚Œãªã„ã®ã§ã€åå‰ã‚’è¦šãˆã¦ã„ãªã„çŠ¶æ…‹ã«ãªã‚‹...!:thinking_face:)

```python
# The only difference is we change the `thread_id` here to "2" instead of "1"

events = graph.stream({"messages": [("user", user_input)]}, {"configurable": {"thread_id": "2"}}, stream_mode="values",)
for event in events:
    event["messages"][-1].pretty_print()
```

```
================================[1m Human Message [0m=================================
Remember my name?
==================================[1m Ai Message [0m==================================
I apologize, but I don't have any previous context or memory of your name. As an AI assistant, I don't retain information from past conversations. Each interaction starts fresh. Could you please tell me your name so I can address you properly in this conversation?
```

By now, we have made a few checkpoints across two different threads.
ã“ã‚Œã¾ã§ã«ã€2ã¤ã®ç•°ãªã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ã«ã‚ãŸã£ã¦ã„ãã¤ã‹ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚
But what goes into a checkpoint?
ã—ã‹ã—ã€**ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ã¯ä½•ãŒå«ã¾ã‚Œã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ**
To inspect a graph's state for a given config at any time, call get_state(config).
**ç‰¹å®šã®è¨­å®šã«å¯¾ã™ã‚‹ã‚°ãƒ©ãƒ•ã®çŠ¶æ…‹ã‚’ã„ã¤ã§ã‚‚ç¢ºèªã™ã‚‹ã«ã¯ã€get_state(config)ã‚’å‘¼ã³å‡ºã—**ã¾ã™ã€‚

```python
snapshot = graph.get_state(config)
snapshot
```

```
StateSnapshot(values={'messages': [HumanMessage(content='Hi there! My name is Will.', additional_kwargs={}, response_metadata={}, id='8c1ca919-c553-4ebf-95d4-b59a2d61e078'), AIMessage(content="Hello Will! It's nice to meet you. How can I assist you today? Is there anything specific you'd like to know or discuss?", additional_kwargs={}, response_metadata={'id': 'msg_01WTQebPhNwmMrmmWojJ9KXJ', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 405, 'output_tokens': 32}}, id='run-58587b77-8c82-41e6-8a90-d62c444a261d-0', usage_metadata={'input_tokens': 405, 'output_tokens': 32, 'total_tokens': 437}), HumanMessage(content='Remember my name?', additional_kwargs={}, response_metadata={}, id='daba7df6-ad75-4d6b-8057-745881cea1ca'), AIMessage(content="Of course, I remember your name, Will. I always try to pay attention to important details that users share with me. Is there anything else you'd like to talk about or any questions you have? I'm here to help with a wide range of topics or tasks.", additional_kwargs={}, response_metadata={'id': 'msg_01E41KitY74HpENRgXx94vag', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 444, 'output_tokens': 58}}, id='run-ffeaae5c-4d2d-4ddb-bd59-5d5cbf2a5af8-0', usage_metadata={'input_tokens': 444, 'output_tokens': 58, 'total_tokens': 502})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7d06e-93e0-6acc-8004-f2ac846575d2'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content="Of course, I remember your name, Will. I always try to pay attention to important details that users share with me. Is there anything else you'd like to talk about or any questions you have? I'm here to help with a wide range of topics or tasks.", additional_kwargs={}, response_metadata={'id': 'msg_01E41KitY74HpENRgXx94vag', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 444, 'output_tokens': 58}}, id='run-ffeaae5c-4d2d-4ddb-bd59-5d5cbf2a5af8-0', usage_metadata={'input_tokens': 444, 'output_tokens': 58, 'total_tokens': 502})]}}, 'step': 4, 'parents': {}}, created_at='2024-09-27T19:30:10.820758+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7d06e-859f-6206-8003-e1bd3c264b8f'}}, tasks=())

```

```python
snapshot.next
# (since the graph ended this turn, `next` is empty. If you fetch a state from within a graph invocation, next tells which node will execute next)
# (ã“ã®ã‚¿ãƒ¼ãƒ³ã§ã‚°ãƒ©ãƒ•ãŒçµ‚äº†ã—ãŸãŸã‚ã€`next`ã¯ç©ºã§ã™ã€‚ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œä¸­ã«çŠ¶æ…‹ã®snapshotã‚’å–å¾—ã™ã‚‹ã¨ã€æ¬¡ã«å®Ÿè¡Œã•ã‚Œã‚‹ãƒãƒ¼ãƒ‰ãŒã‚ã‹ã‚Šã¾ã™)
```

```
()
```

The snapshot above contains the current state values, corresponding config, and the next node to process.
ä¸Šè¨˜ã®**ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã«ã¯ã€ç¾åœ¨ã®state valuesã€å¯¾å¿œã™ã‚‹è¨­å®šã€ãŠã‚ˆã³æ¬¡ã«å‡¦ç†ã™ã‚‹ãƒãƒ¼ãƒ‰ãŒå«ã¾ã‚Œ**ã¦ã„ã‚‹ã€‚
In our case, the graph has reached an END state, so next is empty.
ç§ãŸã¡ã®ã‚±ãƒ¼ã‚¹ã§ã¯ã€ã‚°ãƒ©ãƒ•ã¯ENDçŠ¶æ…‹ã«é”ã—ã¦ã„ã‚‹ãŸã‚ã€nextã¯ç©ºã§ã™ã€‚

Congratulations!
ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼
Your chatbot can now maintain conversation state across sessions thanks to LangGraph's checkpointing system.
ã‚ãªãŸã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯ã€LangGraphã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®ãŠã‹ã’ã§ã€**ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã¾ãŸã„ã§ä¼šè©±ã®çŠ¶æ…‹ã‚’ç¶­æŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸ**ã€‚
This opens up exciting possibilities for more natural, contextual interactions.
ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šè‡ªç„¶ã§æ–‡è„ˆã«æ²¿ã£ãŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®ã‚¨ã‚­ã‚µã‚¤ãƒ†ã‚£ãƒ³ã‚°ãªå¯èƒ½æ€§ãŒé–‹ã‹ã‚Œã¾ã™ã€‚
LangGraph's checkpointing even handles arbitrarily complex graph states, which is much more expressive and powerful than simple chat memory.
LangGraphã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ã€**å˜ç´”ãªãƒãƒ£ãƒƒãƒˆãƒ¡ãƒ¢ãƒªã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«è¡¨ç¾åŠ›è±Šã‹ã§å¼·åŠ›ãªã€ä»»æ„ã®è¤‡é›‘ãªã‚°ãƒ©ãƒ•çŠ¶æ…‹ã‚’å‡¦ç†**ã§ãã‚‹ã€‚

In the next part, we'll introduce human oversight to our bot to handle situations where it may need guidance or verification before proceeding.
æ¬¡ã®ãƒ‘ãƒ¼ãƒˆã§ã¯ã€ãƒœãƒƒãƒˆã«äººé–“ã®ç›£è¦–ã‚’å°å…¥ã—ã€é€²è¡Œã™ã‚‹å‰ã«ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚„æ¤œè¨¼ãŒå¿…è¦ãªçŠ¶æ³ã«å¯¾å‡¦ã—ã¾ã™ã€‚
Check out the code snippet below to review our graph from this section.
ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’ç¢ºèªã—ã¦ã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚°ãƒ©ãƒ•ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚

```python
from typing import Annotated
from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)
tool = TavilySearchResults(max_results=2)
tools = [tool]
llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)
graph_builder.add_conditional_edges("chatbot", tools_condition,)
graph_builder.add_edge("tools", "chatbot")
graph_builder.set_entry_point("chatbot")
graph = graph_builder.compile(checkpointer=memory)
```

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## Part 4: Human-in-the-loop

Agents can be unreliable and may need human input to successfully accomplish tasks.
**ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ä¿¡é ¼ã§ããªã„å ´åˆãŒã‚ã‚Šã€ã‚¿ã‚¹ã‚¯ã‚’æˆåŠŸè£ã«é”æˆã™ã‚‹ãŸã‚ã«äººé–“ã®å…¥åŠ›ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹**ã€‚
Similarly, for some actions, you may want to require human approval before running to ensure that everything is running as intended.
åŒæ§˜ã«ã€**ã„ãã¤ã‹ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦ã¯ã€ã™ã¹ã¦ãŒæ„å›³ã—ãŸé€šã‚Šã«å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã€å®Ÿè¡Œå‰ã«äººé–“ã®æ‰¿èªã‚’å¿…è¦ã¨ã™ã‚‹å ´åˆãŒã‚ã‚‹**ã€‚
LangGraph supports human-in-the-loop workflows in a number of ways.
LangGraphã¯ã€ã•ã¾ã–ã¾ãªæ–¹æ³•ã§äººé–“ãŒä»‹åœ¨ã™ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

In this section, we will use LangGraph's interrupt_before functionality to always break the tool node.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€LangGraphã®`interrupt_before`æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ã€å¸¸ã«ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰ã‚’ä¸­æ–­ã—ã¾ã™ã€‚

First, start from our existing code.
ã¾ãšã€æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚

The following is copied from Part 3.
ä»¥ä¸‹ã¯ã€ãƒ‘ãƒ¼ãƒˆ3ã‹ã‚‰ã‚³ãƒ”ãƒ¼ã—ãŸã‚‚ã®ã§ã™ã€‚

```python
from typing import Annotated
from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from typing_extensions import TypedDict
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

memory = MemorySaver()

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)
tool = TavilySearchResults(max_results=2)
tools = [tool]
llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)
graph_builder.add_conditional_edges("chatbot", tools_condition,)
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")

```

Now, compile the graph, specifying to interrupt_before the tools node.
æ¬¡ã«ã€ã‚°ãƒ©ãƒ•ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã€**ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰ã®å‰ã§ä¸­æ–­ã™ã‚‹ã‚ˆã†ã«**æŒ‡å®šã—ã¾ã™ã€‚

```python
graph = graph_builder.compile(
    checkpointer=memory,  # This is new!
    interrupt_before=["tools"],  # Note: can also interrupt **after** tools, if desired.
    # interrupt_after=["tools"]
)
```

```python
user_input = "I'm learning LangGraph. Could you do some research on it for me?"
config = {"configurable": {"thread_id": "1"}}  # The config is the **second positional argument** to stream() or invoke()!
events = graph.stream({"messages": [("user", user_input)]}, config, stream_mode="values")

for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()
```

```

================================[1m Human Message [0m=================================
I'm learning LangGraph. Could you do some research on it for me?
==================================[1m Ai Message [0m==================================
[{'text': "Certainly! I'd be happy to research LangGraph for you. To get the most up-to-date and comprehensive information, I'll use the Tavily search engine to look this up. Let me do that for you now.", 'type': 'text'}, {'id': 'toolu_01R4ZFcb5hohpiVZwr88Bxhc', 'input': {'query': 'LangGraph framework for building language model applications'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]

```

```

Tool Calls:
tavily_search_results_json (toolu_01R4ZFcb5hohpiVZwr88Bxhc)
Call ID: toolu_01R4ZFcb5hohpiVZwr88Bxhc
Args: query: LangGraph framework for building language model applications

```

```python
snapshot = graph.get_state(config)
snapshot.next
```

```
('tools',)
```

Notice that unlike last time, the "next" node is set to 'tools'.
å‰å›ã¨ã¯ç•°ãªã‚Šã€ã€Œæ¬¡ã€ã®ãƒãƒ¼ãƒ‰ãŒ'tools'ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

We've interrupted here! Let's check the tool invocation.
ã“ã“ã§ä¸­æ–­ã—ã¾ã—ãŸï¼ãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚

```
existing_message = snapshot.values["messages"][-1]
existing_message.tool_calls
```

```
[{'name': 'tavily_search_results_json', 'args': {'query': 'LangGraph framework for building language model applications'}, 'id': 'toolu_01R4ZFcb5hohpiVZwr88Bxhc', 'type': 'tool_call'}]

```

This query seems reasonable.
ã“ã®ã‚¯ã‚¨ãƒªã¯å¦¥å½“ãªã‚ˆã†ã§ã™ã€‚

Nothing to filter here.
ã“ã“ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

The simplest thing the human can do is just let the graph continue executing.
**äººé–“ãŒã§ãã‚‹æœ€ã‚‚ç°¡å˜ãªã“ã¨ã¯ã€ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œã‚’ç¶šã‘ã•ã›ã‚‹ã“ã¨**ã§ã™ã€‚

Let's do that below.
ä»¥ä¸‹ã§ãã‚Œã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚

Next, continue the graph!
æ¬¡ã«ã€ã‚°ãƒ©ãƒ•ã‚’ç¶šè¡Œã—ã¾ã™ï¼

Passing in None will just let the graph continue where it left off, without adding anything new to the state.
**Noneã‚’æ¸¡ã™ã“ã¨ã§ã€ã‚°ãƒ©ãƒ•ã¯æ–°ã—ã„çŠ¶æ…‹ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ãªãã€å…ƒã®ä½ç½®ã‹ã‚‰ç¶šè¡Œã—ã¾ã™**ã€‚

```

# `None` will append nothing new to the current state, letting it resume as if it had never been interrupted

events = graph.stream(None, config, stream_mode="values")

for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()

```

```

==================================[1m Ai Message [0m==================================
[{'text': "Certainly! I'd be happy to research LangGraph for you. To get the most up-to-date and comprehensive information, I'll use the Tavily search engine to look this up. Let me do that for you now.", 'type': 'text'}, {'id': 'toolu_01R4ZFcb5hohpiVZwr88Bxhc', 'input': {'query': 'LangGraph framework for building language model applications'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]

```

```

Tool Calls:
tavily_search_results_json (toolu_01R4ZFcb5hohpiVZwr88Bxhc)
Call ID: toolu_01R4ZFcb5hohpiVZwr88Bxhc
Args: query: LangGraph framework for building language model applications

```

```
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json
[{"url": "https://towardsdatascience.com/from-basics-to-advanced-exploring-langgraph-e8c1cf4db787", "content": "LangChain is one of the leading frameworks for building applications powered by Large Language Models. With the LangChain Expression Language (LCEL), defining and executing step-by-step action sequences â€” also known as chains â€” becomes much simpler. In more technical terms, LangChain allows us to create DAGs (directed acyclic graphs). As LLM applications, particularly LLM agents, have ..."}, {"url": "https://github.com/langchain-ai/langgraph", "content": "Overview. LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures ..."}]
```

Congrats! You've used an interrupt to add human-in-the-loop execution to your chatbot, allowing for human oversight and intervention when needed.
ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼ã‚ãªãŸã¯**interruptã‚’ä½¿ç”¨ã—ã¦ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã«äººé–“ãŒä»‹åœ¨ã™ã‚‹å®Ÿè¡Œã‚’è¿½åŠ ã—ã€å¿…è¦ã«å¿œã˜ã¦äººé–“ã®ç›£è¦–ã¨ä»‹å…¥ã‚’å¯èƒ½ã«ã—ãŸ**ã€‚

This opens up the potential UIs you can create with your AI systems.
ã“ã‚Œã«ã‚ˆã‚Šã€AIã‚·ã‚¹ãƒ†ãƒ ã§ä½œæˆã§ãã‚‹æ½œåœ¨çš„ãªUIãŒåºƒãŒã‚Šã¾ã™ã€‚

Since we have already added a checkpointer, the graph can be paused indefinitely and resumed at any time as if nothing had happened.
**ã™ã§ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ã‚¿ãƒ¼ã‚’è¿½åŠ ã—ã¦ã„ã‚‹ãŸã‚ã€ã‚°ãƒ©ãƒ•ã¯ç„¡æœŸé™ã«ä¸€æ™‚åœæ­¢ã§ãã€ä½•ã‚‚èµ·ã“ã‚‰ãªã‹ã£ãŸã‹ã®ã‚ˆã†ã«ã„ã¤ã§ã‚‚å†é–‹ã§ãã¾ã™**ã€‚

Next, we'll explore how to further customize the bot's behavior using custom state updates.
æ¬¡ã«ã€ã‚«ã‚¹ã‚¿ãƒ çŠ¶æ…‹æ›´æ–°ã‚’ä½¿ç”¨ã—ã¦ãƒœãƒƒãƒˆã®å‹•ä½œã‚’ã•ã‚‰ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹æ–¹æ³•ã‚’æ¢ã‚Šã¾ã™ã€‚

Below is a copy of the code you used in this section.
ä»¥ä¸‹ã¯ã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ä½¿ç”¨ã—ãŸã‚³ãƒ¼ãƒ‰ã®ã‚³ãƒ”ãƒ¼ã§ã™ã€‚

The only difference between this and the previous parts is the addition of the interrupt_before argument.
ã“ã‚Œã¨å‰ã®éƒ¨åˆ†ã¨ã®å”¯ä¸€ã®é•ã„ã¯ã€interrupt_beforeå¼•æ•°ã®è¿½åŠ ã§ã™ã€‚

```

from typing import Annotated
from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)
tool = TavilySearchResults(max_results=2)
tools = [tool]
llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)
graph_builder.add_conditional_edges("chatbot", tools_condition,)
graph_builder.add_edge("tools", "chatbot")
graph_builder.set_entry_point("chatbot")

memory = MemorySaver()
graph = graph_builder.compile(
    checkpointer=memory,  # This is new!
    interrupt_before=["tools"],  # Note: can also interrupt **after** actions, if desired.
    # interrupt_after=["tools"]
)

```

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## Part 5: Manually Updating the StateÂ¶

## ç¬¬5éƒ¨: çŠ¶æ…‹ã®æ‰‹å‹•æ›´æ–°

In the previous section, we showed how to interrupt a graph so that a human could inspect its actions.
å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã‚°ãƒ©ãƒ•ã‚’ä¸­æ–­ã—ã¦äººé–“ãŒãã®å‹•ä½œã‚’æ¤œæŸ»ã§ãã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã—ãŸã€‚

This lets the human read the state, but if they want to change their agent's course, they'll need to have write access.
ã“ã‚Œã«ã‚ˆã‚Šã€äººé–“ã¯çŠ¶æ…‹ã‚’èª­ã¿å–ã‚‹ã“ã¨ãŒã§ãã¾ã™ãŒã€**ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®é€²è·¯ã‚’å¤‰æ›´ã—ãŸã„å ´åˆã¯ã€æ›¸ãè¾¼ã¿ã‚¢ã‚¯ã‚»ã‚¹ãŒå¿…è¦**ã§ã™ã€‚

Thankfully, LangGraph lets you manually update state!
å¹¸ã„ãªã“ã¨ã«ã€**LangGraphã§ã¯çŠ¶æ…‹ã‚’æ‰‹å‹•ã§æ›´æ–°ã§ãã¾ã™**ï¼

Updating the state lets you control the agent's trajectory by modifying its actions (even modifying the past!).
çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹ã“ã¨ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è»Œé“ã‚’åˆ¶å¾¡ã—ã€ãã®è¡Œå‹•ã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼ˆéå»ã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã•ãˆå¯èƒ½ã§ã™ï¼ï¼‰ã€‚

This capability is particularly useful when you want to correct the agent's mistakes, explore alternative paths, or guide the agent towards a specific goal.
ã“ã®æ©Ÿèƒ½ã¯ã€**ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®èª¤ã‚Šã‚’ä¿®æ­£ã—ãŸã‚Šã€ä»£æ›¿ã®é“ã‚’æ¢ã£ãŸã‚Šã€ç‰¹å®šã®ç›®æ¨™ã«å‘ã‹ã£ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å°ã„ãŸã‚Šã™ã‚‹éš›ã«ç‰¹ã«ä¾¿åˆ©**ã§ã™ã€‚

We'll show how to update a checkpointed state below.
ä»¥ä¸‹ã«ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆçŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚

As before, first, define your graph.
å‰ã¨åŒæ§˜ã«ã€ã¾ãšã‚°ãƒ©ãƒ•ã‚’å®šç¾©ã—ã¾ã™ã€‚

We'll reuse the exact same graph as before.
å‰ã¨å…¨ãåŒã˜ã‚°ãƒ©ãƒ•ã‚’å†åˆ©ç”¨ã—ã¾ã™ã€‚

```python
from typing import Annotated
from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from typing_extensions import TypedDict
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)
tool = TavilySearchResults(max_results=2)
tools = [tool]
llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)
graph_builder.add_conditional_edges("chatbot", tools_condition,)
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
memory = MemorySaver()
graph = graph_builder.compile(checkpointer=memory,  # This is new!
                               interrupt_before=["tools"],  # Note: can also interrupt **after** actions, if desired.
                               # interrupt_after=["tools"])
user_input = "I'm learning LangGraph. Could you do some research on it for me?"
config = {"configurable": {"thread_id": "1"}}  # The config is the **second positional argument** to stream() or invoke()!
events = graph.stream({"messages": [("user", user_input)]}, config)

for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()
```

```python
snapshot = graph.get_state(config)
existing_message = snapshot.values["messages"][-1]
existing_message.pretty_print()

```

```

==================================[1m Ai Message [0m==================================
[{'text': "Certainly! I'd be happy to research LangGraph for you. To get the most up-to-date and comprehensive information, I'll use the Tavily search engine to look this up. Let me do that for you now.", 'type': 'text'}, {'id': 'toolu_018YcbFR37CG8RRXnavH5fxZ', 'input': {'query': 'LangGraph: what is it, how is it used in AI development'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls: tavily_search_results_json (toolu_018YcbFR37CG8RRXnavH5fxZ)
Call ID: toolu_018YcbFR37CG8RRXnavH5fxZ
Args: query: LangGraph: what is it, how is it used in AI development

```

But what if the user wants to intercede?
ã—ã‹ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä»‹å…¥ã—ãŸã„å ´åˆã¯ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿ

What if we think the chat bot doesn't need to use the tool?
ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãŒãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒãªã„ã¨è€ƒãˆãŸå ´åˆã¯ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿ

Let's directly provide the correct response!
æ­£ã—ã„å¿œç­”ã‚’ç›´æ¥æä¾›ã—ã¾ã—ã‚‡ã†ï¼

```python
from langchain_core.messages import AIMessage, ToolMessage

answer = ("LangGraph is a library for building stateful, multi-actor applications with LLMs.")
new_messages = [
    # The LLM API expects some ToolMessage to match its tool call. We'll satisfy that here.
    ToolMessage(content=answer, tool_call_id=existing_message.tool_calls[0]["id"]),
    # And then directly "put words in the LLM's mouth" by populating its response.
    AIMessage(content=answer),
]

new_messages[-1].pretty_print()
graph.update_state(
    # Which state to update
    config,
    # The updated values to provide. The messages in our `State` are "append-only", meaning this will be appended
    # to the existing state. We will review how to update existing messages in the next section!
    {"messages": new_messages},
)

print("\n\nLast 2 messages;")
print(graph.get_state(config).values["messages"][-2:])

```

```

==================================[1m Ai Message [0m==================================
LangGraph is a library for building stateful, multi-actor applications with LLMs.
Last 2 messages;
[ToolMessage(content='LangGraph is a library for building stateful, multi-actor applications with LLMs.', id='675f7618-367f-44b7-b80e-2834afb02ac5', tool_call_id='toolu_018YcbFR37CG8RRXnavH5fxZ'), AIMessage(content='LangGraph is a library for building stateful, multi-actor applications with LLMs.', additional_kwargs={}, response_metadata={}, id='35fd5682-0c2a-4200-b192-71c59ac6d412')]

```

Now the graph is complete, since we've provided the final response message!
ã“ã‚Œã§ã‚°ãƒ©ãƒ•ã¯å®Œæˆã—ã¾ã—ãŸã€‚æœ€çµ‚çš„ãªå¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æä¾›ã—ãŸã‹ã‚‰ã§ã™ï¼

Since state updates simulate a graph step, they even generate corresponding traces.
çŠ¶æ…‹ã®æ›´æ–°ã¯ã‚°ãƒ©ãƒ•ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã€å¯¾å¿œã™ã‚‹ãƒˆãƒ¬ãƒ¼ã‚¹ã‚‚ç”Ÿæˆã•ã‚Œã¾ã™ã€‚

Inspect the LangSmith trace of the update_state call above to see what's going on.
ä¸Šè¨˜ã®update_stateå‘¼ã³å‡ºã—ã®LangSmithãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ç¢ºèªã—ã¦ã€ä½•ãŒèµ·ã“ã£ã¦ã„ã‚‹ã‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

Notice that our new messages are appended to the messages already in the state.
æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã™ã§ã«çŠ¶æ…‹ã«ã‚ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

Remember how we defined the State type?
Stateå‹ã‚’ã©ã®ã‚ˆã†ã«å®šç¾©ã—ãŸã‹ã‚’æ€ã„å‡ºã—ã¦ãã ã•ã„ã€‚

```

class State(TypedDict):
    messages: Annotated[list, add_messages]

```

We annotated messages with the pre-built add_messages function.
ç§ãŸã¡ã¯ã€**pre-builtã®add_messagesé–¢æ•°ã§messagesã‚’æ³¨é‡ˆã—ã¾ã—ãŸ**ã€‚
This instructs the graph to always append values to the existing list, rather than overwriting the list directly.
**ã“ã‚Œã«ã‚ˆã‚Šã€ã‚°ãƒ©ãƒ•ã¯ãƒªã‚¹ãƒˆã‚’ç›´æ¥ä¸Šæ›¸ãã™ã‚‹ã®ã§ã¯ãªãã€å¸¸ã«æ—¢å­˜ã®ãƒªã‚¹ãƒˆã«å€¤ã‚’è¿½åŠ ã™ã‚‹ã‚ˆã†ã«æŒ‡ç¤º**ã—ã¾ã™ã€‚

The same logic is applied here, so the messages we passed to update_state were appended in the same way!
ã“ã“ã§ã‚‚åŒã˜è«–ç†ãŒé©ç”¨ã•ã‚Œã‚‹ãŸã‚ã€update_stateã«æ¸¡ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯åŒã˜ã‚ˆã†ã«è¿½åŠ ã•ã‚Œã¾ã—ãŸï¼

The update_state function operates as if it were one of the nodes in your graph!
**update_stateé–¢æ•°ã¯ã€ã‚°ãƒ©ãƒ•å†…ã®ãƒãƒ¼ãƒ‰ã®1ã¤ã§ã‚ã‚‹ã‹ã®ã‚ˆã†ã«å‹•ä½œã—ã¾ã™**ï¼

By default, the update operation uses the node that was last executed, but you can manually specify it below.
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€æ›´æ–°æ“ä½œã¯æœ€å¾Œã«å®Ÿè¡Œã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€ä»¥ä¸‹ã§æ‰‹å‹•ã§æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

Let's add an update and tell the graph to treat it as if it came from the "chatbot".
æ›´æ–°ã‚’è¿½åŠ ã—ã€ã‚°ãƒ©ãƒ•ã«ã€Œãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€ã‹ã‚‰ã®ã‚‚ã®ã§ã‚ã‚‹ã‹ã®ã‚ˆã†ã«æ‰±ã†ã‚ˆã†ã«æŒ‡ç¤ºã—ã¾ã—ã‚‡ã†ã€‚

```

graph.update_state(config, {"messages": [AIMessage(content="I'm an AI expert!")]},  # Which node for this function to act as. It will automatically continue
                      # processing as if this node just ran.
                      as_node="chatbot",
)

```

```

{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7d134-3958-6412-8002-3f4b4112062f'}}

```

Check out the LangSmith trace for this update call at the provided link.
æä¾›ã•ã‚ŒãŸãƒªãƒ³ã‚¯ã§ã“ã®æ›´æ–°å‘¼ã³å‡ºã—ã®LangSmithãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

Notice from the trace that the graph continues into the tools_condition edge.
ãƒˆãƒ¬ãƒ¼ã‚¹ã‹ã‚‰ã€ã‚°ãƒ©ãƒ•ãŒtools_conditionã‚¨ãƒƒã‚¸ã«ç¶šã„ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

We just told the graph to treat the update as_node="chatbot".
ç§ãŸã¡ã¯ã€**ã‚°ãƒ©ãƒ•ã«æ›´æ–°ã‚’as_node="chatbot"ã¨ã—ã¦æ‰±ã†ã‚ˆã†ã«**æŒ‡ç¤ºã—ã¾ã—ãŸã€‚

If we follow the diagram below and start from the chatbot node, we naturally end up in the tools_condition edge and then **end** since our updated message lacks tool calls.
ä»¥ä¸‹ã®å›³ã«å¾“ã„ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒãƒ¼ãƒ‰ã‹ã‚‰å§‹ã‚ã‚‹ã¨ã€è‡ªç„¶ã«tools_conditionã‚¨ãƒƒã‚¸ã«åˆ°é”ã—ã€ãã®å¾Œ__end__ã«è‡³ã‚Šã¾ã™ã€‚æ›´æ–°ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã¯ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒãªã„ãŸã‚ã§ã™ã€‚

```

from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:  # This requires some extra dependencies and is optional
    pass

```

Inspect the current state as before to confirm the checkpoint reflects our manual updates.
ä»¥å‰ã¨åŒæ§˜ã«ç¾åœ¨ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒæ‰‹å‹•æ›´æ–°ã‚’åæ˜ ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```

snapshot = graph.get_state(config)
print(snapshot.values["messages"][-3:])
print(snapshot.next)

```

```

[ToolMessage(content='LangGraph is a library for building stateful, multi-actor applications with LLMs.', id='675f7618-367f-44b7-b80e-2834afb02ac5', tool_call_id='toolu_018YcbFR37CG8RRXnavH5fxZ'), AIMessage(content='LangGraph is a library for building stateful, multi-actor applications with LLMs.', additional_kwargs={}, response_metadata={}, id='35fd5682-0c2a-4200-b192-71c59ac6d412'), AIMessage(content="I'm an AI expert!", additional_kwargs={}, response_metadata={}, id='288e2f74-f1cb-4082-8c3c-af4695c83117')]()

```

The add_messages function we used to annotate our graph's State above controls how updates are made to the messages key.
ä¸Šè¨˜ã§**ã‚°ãƒ©ãƒ•ã®Stateã‚’æ³¨é‡ˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã—ãŸadd_messagesé–¢æ•°ã¯ã€messagesã‚­ãƒ¼ã¸ã®æ›´æ–°ãŒã©ã®ã‚ˆã†ã«è¡Œã‚ã‚Œã‚‹ã‹ã‚’åˆ¶å¾¡**ã—ã¾ã™ã€‚

This function looks at any message IDs in the new_messages list.
ã“ã®é–¢æ•°ã¯ã€æ–°ã—ã„messagesãƒªã‚¹ãƒˆå†…ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸IDã‚’ç¢ºèªã—ã¾ã™ã€‚

If the ID matches a message in the existing state, add_messages overwrites the existing message with the new content.
**IDãŒæ—¢å­˜ã®çŠ¶æ…‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€è‡´ã™ã‚‹å ´åˆã€add_messagesã¯æ—¢å­˜ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§ä¸Šæ›¸ã**ã—ã¾ã™ã€‚

As an example, let's update the tool invocation to make sure we get good results from our search engine!
ä¾‹ã¨ã—ã¦ã€ãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—ã‚’æ›´æ–°ã—ã¦ã€æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‹ã‚‰è‰¯ã„çµæœã‚’å¾—ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã—ã¾ã—ã‚‡ã†ï¼

First, start a new thread:
ã¾ãšã€æ–°ã—ã„ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚

```python
user_input = "I'm learning LangGraph. Could you do some research on it for me?"
config = {"configurable": {"thread_id": "2"}}  # we'll use thread_id = 2 here
events = graph.stream({"messages": [("user", user_input)]}, config, stream_mode="values")

for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()

```

```

================================[1m Human Message [0m=================================
I'm learning LangGraph. Could you do some research on it for me?
==================================[1m Ai Message [0m==================================
[{'text': "Certainly! I'd be happy to research LangGraph for you. To get the most up-to-date and accurate information, I'll use the Tavily search engine to look this up. Let me do that for you now.", 'type': 'text'}, {'id': 'toolu_01TfAeisrpx4ddgJpoAxqrVh', 'input': {'query': 'LangGraph framework for language models'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls: tavily_search_results_json (toolu_01TfAeisrpx4ddgJpoAxqrVh)
Call ID: toolu_01TfAeisrpx4ddgJpoAxqrVh
Args: query: LangGraph framework for language models

```

```python
from langchain_core.messages import AIMessage

snapshot = graph.get_state(config)
existing_message = snapshot.values["messages"][-1]
print("Original")
print("Message ID", existing_message.id)
print(existing_message.tool_calls[0])

new_tool_call = existing_message.tool_calls[0].copy()
new_tool_call["args"]["query"] = "LangGraph human-in-the-loop workflow"

new_message = AIMessage(content=existing_message.content, tool_calls=[new_tool_call],  # Important! The ID is how LangGraph knows to REPLACE the message in the state rather than APPEND this message
                        id=existing_message.id,)

print("Updated")
print(new_message.tool_calls[0])
print("Message ID", new_message.id)

graph.update_state(config, {"messages": [new_message]})
print("\n\nTool calls")
graph.get_state(config).values["messages"][-1].tool_calls
```

```
Original
Message ID run-342f3f54-356b-4cc1-b747-573f6aa31054-0
{'name': 'tavily_search_results_json', 'args': {'query': 'LangGraph framework for language models'}, 'id': 'toolu_01TfAeisrpx4ddgJpoAxqrVh', 'type': 'tool_call'}
Updated
{'name': 'tavily_search_results_json', 'args': {'query': 'LangGraph human-in-the-loop workflow'}, 'id': 'toolu_01TfAeisrpx4ddgJpoAxqrVh', 'type': 'tool_call'}
Message ID run-342f3f54-356b-4cc1-b747-573f6aa31054-0
Tool calls

```

```

[{'name': 'tavily_search_results_json', 'args': {'query': 'LangGraph human-in-the-loop workflow'}, 'id': 'toolu_01TfAeisrpx4ddgJpoAxqrVh', 'type': 'tool_call'}]

```

Notice that we've modified the AI's tool invocation to search for "LangGraph human-in-the-loop workflow" instead of the simple "LangGraph".
AIã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å˜ç´”ãªã€ŒLangGraphã€ã§ã¯ãªãã€ŒLangGraph human-in-the-loop workflowã€ã‚’æ¤œç´¢ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ã—ãŸã“ã¨ã«æ³¨ç›®ã—ã¦ãã ã•ã„ã€‚

Check out the LangSmith trace to see the state update call - you can see our new message has successfully updated the previous AI message.
çŠ¶æ…‹æ›´æ–°å‘¼ã³å‡ºã—ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«LangSmithãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒä»¥å‰ã®AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ­£å¸¸ã«æ›´æ–°ã—ãŸã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

Resume the graph by streaming with an input of None and the existing config.
Noneã®å…¥åŠ›ã¨æ—¢å­˜ã®è¨­å®šã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã—ã¦ã‚°ãƒ©ãƒ•ã‚’å†é–‹ã—ã¾ã™ã€‚

```

events = graph.stream(None, config, stream_mode="values")

for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()

```

```shell
==================================[1m Ai Message [0m==================================
[{'text': "Certainly! I'd be happy to research LangGraph for you. To get the most up-to-date and accurate information, I'll use the Tavily search engine to look this up. Let me do that for you now.", 'type': 'text'}, {'id': 'toolu_01TfAeisrpx4ddgJpoAxqrVh', 'input': {'query': 'LangGraph framework for language models'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls: tavily_search_results_json (toolu_01TfAeisrpx4ddgJpoAxqrVh)
Call ID: toolu_01TfAeisrpx4ddgJpoAxqrVh
Args: query: LangGraph human-in-the-loop workflow
==================================[1m Tool Message [0m=================================
Name: tavily_search_results_json
[{"url": "https://www.youtube.com/watch?v=9BPCV5TYPmg", "content": "In this video, I'll show you how to handle persistence with LangGraph, enabling a unique Human-in-the-Loop workflow. This approach allows a human to grant an..."}, {"url": "https://medium.com/@kbdhunga/implementing-human-in-the-loop-with-langgraph-ccfde023385c", "content": "Implementing a Human-in-the-Loop (HIL) framework in LangGraph with the Streamlit app provides a robust mechanism for user engagement and decision-making. By incorporating breakpoints and ..."}]
==================================[1m Ai Message [0m==================================
Thank you for your patience. I've found some information about LangGraph, particularly focusing on its human-in-the-loop workflow capabilities. Let me summarize what I've learned for you:

1. LangGraph Overview: LangGraph is a framework for building stateful, multi-actor applications with Large Language Models (LLMs). It's particularly useful for creating complex, interactive AI systems.
2. Human-in-the-Loop (HIL) Workflow: One of the key features of LangGraph is its support for human-in-the-loop workflows. This means that it allows for human intervention and decision-making within AI-driven processes.
3. Persistence Handling: LangGraph offers capabilities for handling persistence, which is crucial for maintaining state across interactions in a workflow.
4. Implementation with Streamlit: There are examples of implementing LangGraph's human-in-the-loop functionality using Streamlit, a popular Python library for creating web apps. This combination allows for the creation of interactive user interfaces for AI applications.
5. Breakpoints and User Engagement: LangGraph allows the incorporation of breakpoints in the workflow. These breakpoints are points where the system can pause and wait for human input or decision-making, enhancing user engagement and control over the AI process.
6. Decision-Making Mechanism: The human-in-the-loop framework in LangGraph provides a robust mechanism for integrating user decision-making into AI workflows. This is particularly useful in scenarios where human judgment or expertise is needed to guide or validate AI actions.
7. Flexibility and Customization: From the information available, it seems that LangGraph offers flexibility in how human-in-the-loop processes are implemented, allowing developers to customize the interaction points and the nature of human involvement based on their specific use case.

LangGraph appears to be a powerful tool for developers looking to create more interactive and controllable AI applications, especially those that benefit from human oversight or input at crucial stages of the process.
LangGraphã¯ã€ç‰¹ã«ãƒ—ãƒ­ã‚»ã‚¹ã®é‡è¦ãªæ®µéšã§äººé–“ã®ç›£è¦–ã‚„å…¥åŠ›ãŒæœ‰ç›ŠãªAIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚ˆã‚Šã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã§åˆ¶å¾¡å¯èƒ½ã«ã™ã‚‹ãŸã‚ã«ã€é–‹ç™ºè€…ã«ã¨ã£ã¦å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã®ã‚ˆã†ã§ã™ã€‚

Would you like me to research any specific aspect of LangGraph in more detail, or do you have any questions about what I've found so far?
LangGraphã®ç‰¹å®šã®å´é¢ã«ã¤ã„ã¦ã•ã‚‰ã«è©³ã—ãèª¿æŸ»ã—ã¦ã»ã—ã„ã§ã™ã‹ã€ãã‚Œã¨ã‚‚ç§ãŒã“ã‚Œã¾ã§ã«è¦‹ã¤ã‘ãŸã“ã¨ã«ã¤ã„ã¦è³ªå•ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ
```

All of this is reflected in the graph's checkpointed memory, meaning if we continue the conversation, it will recall all the modified state.
ã“ã‚Œã‚‰ã™ã¹ã¦ã¯ã‚°ãƒ©ãƒ•ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ¡ãƒ¢ãƒªã«åæ˜ ã•ã‚Œã¦ãŠã‚Šã€ä¼šè©±ã‚’ç¶šã‘ã‚‹ã¨ã€ã™ã¹ã¦ã®å¤‰æ›´ã•ã‚ŒãŸçŠ¶æ…‹ã‚’è¨˜æ†¶ã—ã¾ã™ã€‚

```python
events = graph.stream({"messages": ("user", "Remember what I'm learning about?",)}, config, stream_mode="values",)

for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()
```

```
================================[1m Human Message [0m=================================
Remember what I'm learning about?
==================================[1m Ai Message [0m==================================
I apologize for my oversight. You're absolutely right to remind me. You mentioned that you're learning LangGraph. Thank you for bringing that back into focus.
Since you're in the process of learning LangGraph, it would be helpful to know more about your current level of understanding and what specific aspects of LangGraph you're most interested in or finding challenging.
This way, I can provide more targeted information or explanations that align with your learning journey.
Are there any particular areas of LangGraph you'd like to explore further?
For example:

1. Basic concepts and architecture of LangGraph
2. Setting up and getting started with LangGraph
3. Implementing specific features like the human-in-the-loop workflow
4. Best practices for using LangGraph in projects
5. Comparisons with other similar frameworks
Or if you have any specific questions about what you've learned so far, I'd be happy to help clarify or expand on those topics.
Please let me know what would be most useful for your learning process.
```

The graph code for this section is identical to previous ones.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚°ãƒ©ãƒ•ã‚³ãƒ¼ãƒ‰ã¯ã€ä»¥å‰ã®ã‚‚ã®ã¨åŒã˜ã§ã™ã€‚

The key snippets to remember are to add .compile(..., interrupt_before=[...]) (or interrupt_after) if you want to explicitly pause the graph whenever it reaches a node.
**ãƒãƒ¼ãƒ‰ã«åˆ°é”ã™ã‚‹ãŸã³ã«ã‚°ãƒ©ãƒ•ã‚’æ˜ç¤ºçš„ã«ä¸€æ™‚åœæ­¢ã—ãŸã„å ´åˆ**ã¯ã€.compile(..., interrupt_before=[...])ï¼ˆã¾ãŸã¯interrupt_afterï¼‰ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚

Then you can use update_state to modify the checkpoint and control how the graph should proceed.
ãã®å¾Œã€update_stateã‚’ä½¿ç”¨ã—ã¦ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å¤‰æ›´ã—ã€ã‚°ãƒ©ãƒ•ãŒã©ã®ã‚ˆã†ã«é€²è¡Œã™ã‚‹ã‹ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## Part 6: Customizing StateÂ¶ çŠ¶æ…‹ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

So far, we've relied on a simple state (it's just a list of messages!).
ã“ã‚Œã¾ã§ã€ç§ãŸã¡ã¯**ã‚·ãƒ³ãƒ—ãƒ«ãªçŠ¶æ…‹ï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã ã‘ï¼‰**ã«ä¾å­˜ã—ã¦ãã¾ã—ãŸã€‚
You can go far with this simple state, but if you want to define complex behavior without relying on the message list, you can add additional fields to the state.
**ã“ã®ã‚·ãƒ³ãƒ—ãƒ«ãªçŠ¶æ…‹ã§å¤šãã®ã“ã¨ã‚’é”æˆã§ãã¾ã™ãŒã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã«ä¾å­˜ã›ãšã«è¤‡é›‘ãªå‹•ä½œã‚’å®šç¾©ã—ãŸã„å ´åˆã¯ã€çŠ¶æ…‹ã«è¿½åŠ ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ ã§ãã¾ã™**ã€‚
In this section, we will extend our chat bot with a new node to illustrate this.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã“ã‚Œã‚’ç¤ºã™ãŸã‚ã«æ–°ã—ã„ãƒãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’æ‹¡å¼µã—ã¾ã™ã€‚

In the examples above, we involved a human deterministically: the graph always interrupted whenever a tool was invoked.
ä¸Šè¨˜ã®ä¾‹ã§ã¯ã€ç§ãŸã¡ã¯äººé–“ã‚’æ±ºå®šè«–çš„ã«é–¢ä¸ã•ã›ã¾ã—ãŸï¼šãƒ„ãƒ¼ãƒ«ãŒå‘¼ã³å‡ºã•ã‚Œã‚‹ãŸã³ã«ã‚°ãƒ©ãƒ•ã¯å¸¸ã«ä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚
Suppose we wanted our chat bot to have the choice of relying on a human.
**ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãŒäººé–“ã«ä¾å­˜ã™ã‚‹é¸æŠè‚¢ã‚’æŒã¤**ã‚ˆã†ã«ã—ãŸã„ã¨ã—ã¾ã—ã‚‡ã†ã€‚

One way to do this is to create a passthrough "human" node, before which the graph will always stop.
ã“ã‚Œã‚’è¡Œã†ä¸€ã¤ã®æ–¹æ³•ã¯ã€**ã‚°ãƒ©ãƒ•ãŒå¸¸ã«åœæ­¢ã™ã‚‹passthroughã¨ã—ã¦ã€Œäººé–“ã€ãƒãƒ¼ãƒ‰ã‚’ä½œæˆã™ã‚‹ã“ã¨**ã§ã™ã€‚
We will only execute this node if the LLM invokes a "human" tool.
ã“ã®ãƒãƒ¼ãƒ‰ã¯ã€LLMãŒã€Œäººé–“ã€ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã—ãŸå ´åˆã«ã®ã¿å®Ÿè¡Œã—ã¾ã™ã€‚
For our convenience, we will include an "ask_human" flag in our graph state that we will flip if the LLM calls this tool.
ç§ãŸã¡ã®ä¾¿å®œã®ãŸã‚ã«ã€**LLMãŒã“ã®ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã—ãŸå ´åˆã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã€Œask_humanã€ãƒ•ãƒ©ã‚°ã‚’ã‚°ãƒ©ãƒ•çŠ¶æ…‹ã«å«ã‚ã‚‹**ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

Below, define this new graph, with an updated State
ä»¥ä¸‹ã«ã€æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹ã‚’æŒã¤æ–°ã—ã„ã‚°ãƒ©ãƒ•ã‚’å®šç¾©ã—ã¾ã™ã€‚

```python
from typing import Annotated
from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from typing_extensions import TypedDict
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

class State(TypedDict):
    messages: Annotated[list, add_messages]  
    # This flag is new
    ask_human: bool
```

Next, define a schema to show the model to let it decide to request assistance.
æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã«æ”¯æ´ã‚’è¦æ±‚ã™ã‚‹ã‹ã©ã†ã‹ã‚’æ±ºå®šã•ã›ã‚‹ãŸã‚ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©ã—ã¾ã™ã€‚
Using Pydantic with LangChain
LangChainã¨å…±ã«Pydanticã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3.
ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€langchain-core >= 0.3ã‚’å¿…è¦ã¨ã™ã‚‹Pydantic v2 BaseModelã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.
langchain-core < 0.3ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€Pydantic v1ã¨v2 BaseModelsã®æ··åœ¨ã«ã‚ˆã‚Šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚

```python
from pydantic import BaseModel

class RequestAssistance(BaseModel):
    """
    ä¼šè©±ã‚’å°‚é–€å®¶ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¾ã™ã€‚
    ç›´æ¥æ”¯æ´ã§ããªã„å ´åˆã‚„ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚ãªãŸã®æ¨©é™ã‚’è¶…ãˆãŸæ”¯æ´ã‚’å¿…è¦ã¨ã™ã‚‹å ´åˆã«ä½¿ç”¨ã—ã¾ã™ã€‚
    ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ãƒ¦ãƒ¼ã‚¶ã‚’ä¸­ç¶™ã—ã¦ã€å°‚é–€å®¶ãŒé©åˆ‡ãªã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’æä¾›ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
    """
    request: str
```

Next, define the chatbot node.
æ¬¡ã«ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒãƒ¼ãƒ‰ã‚’å®šç¾©ã—ã¾ã™ã€‚
The primary modification here is flip the ask_human flag if we see that the chat bot has invoked the RequestAssistance flag.
ã“ã“ã§ã®ä¸»ãªå¤‰æ›´ã¯ã€**ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãŒRequestAssistanceãƒ•ãƒ©ã‚°ã‚’å‘¼ã³å‡ºã—ãŸå ´åˆã«ask_humanãƒ•ãƒ©ã‚°ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨**ã§ã™ã€‚

```python
tool = TavilySearchResults(max_results=2)
tools = [tool]
llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")
# We can bind the llm to a tool definition, a pydantic model, or a json schema
# ç§ãŸã¡ã¯llmã‚’ãƒ„ãƒ¼ãƒ«å®šç¾©ã€pydanticãƒ¢ãƒ‡ãƒ«ã€ã¾ãŸã¯jsonã‚¹ã‚­ãƒ¼ãƒã«ãƒã‚¤ãƒ³ãƒ‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
llm_with_tools = llm.bind_tools(tools + [RequestAssistance])

def chatbot(state: State):
    response = llm_with_tools.invoke(state["messages"])
    ask_human = False
    if (response.tool_calls and response.tool_calls[0]["name"] == RequestAssistance.__name__):
        ask_human = True
    return {
        "messages": [response],
        "ask_human": ask_human
    }
```

Next, create the graph builder and add the chatbot and tools nodes to the graph, same as before.
æ¬¡ã«ã€ã‚°ãƒ©ãƒ•ãƒ“ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆã—ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¨ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰ã‚’ã‚°ãƒ©ãƒ•ã«è¿½åŠ ã—ã¾ã™ã€‚ä»¥å‰ã¨åŒæ§˜ã§ã™ã€‚

```

graph_builder = StateGraph(State)
graph_builder.add_node("chatbot", chatbot)
graph_builder.add_node("tools", ToolNode(tools=[tool]))

```

Next, create the "human" node.
æ¬¡ã«ã€ã€Œäººé–“ã€ãƒãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¾ã™ã€‚
This node function is mostly a placeholder in our graph that will trigger an interrupt.
ã“ã®ãƒãƒ¼ãƒ‰é–¢æ•°ã¯ã€ä¸»ã«ä¸­æ–­ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã§ã™ã€‚
If the human does not manually update the state during the interrupt, it inserts a tool message so the LLM knows the user was requested but didn't respond.
äººé–“ãŒä¸­æ–­ä¸­ã«æ‰‹å‹•ã§çŠ¶æ…‹ã‚’æ›´æ–°ã—ãªã„å ´åˆã€ãƒ„ãƒ¼ãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŒ¿å…¥ã—ã¦ã€LLMãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¦æ±‚ã•ã‚ŒãŸãŒå¿œç­”ã—ãªã‹ã£ãŸã“ã¨ã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
This node also unsets the ask_human flag so the graph knows not to revisit the node unless further requests are made.
ã“ã®ãƒãƒ¼ãƒ‰ã¯ã¾ãŸã€ask_humanãƒ•ãƒ©ã‚°ã‚’è§£é™¤ã—ã€ã•ã‚‰ãªã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒãªã„é™ã‚Šãƒãƒ¼ãƒ‰ã‚’å†è¨ªã—ãªã„ã“ã¨ã‚’ã‚°ãƒ©ãƒ•ã«çŸ¥ã‚‰ã›ã¾ã™ã€‚

```python
from langchain_core.messages import AIMessage, ToolMessage

def create_response(response: str, ai_message: AIMessage):
    return ToolMessage(content=response, tool_call_id=ai_message.tool_calls[0]["id"],)

def human_node(state: State):
    new_messages = []
    if not isinstance(state["messages"][-1], ToolMessage):
        # Typically, the user will have updated the state during the interrupt.
        # é€šå¸¸ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ä¸­æ–­ä¸­ã«çŠ¶æ…‹ã‚’æ›´æ–°ã—ã¦ã„ã‚‹ã§ã—ã‚‡ã†ã€‚
        # If they choose not to, we will include a placeholder ToolMessage to let the LLM continue.
        # ã‚‚ã—æ›´æ–°ã—ãªã„ã“ã¨ã‚’é¸æŠã—ãŸå ´åˆã€LLMãŒç¶šè¡Œã§ãã‚‹ã‚ˆã†ã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã®ToolMessageã‚’å«ã‚ã¾ã™ã€‚
        new_messages.append(create_response("No response from human.", state["messages"][-1]))
    return {
        # Append the new messages
        "messages": new_messages,
        # Unset the flag
        "ask_human": False,
    }

graph_builder.add_node("human", human_node)
```

Next, define the conditional logic.
æ¬¡ã«ã€æ¡ä»¶ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®šç¾©ã—ã¾ã™ã€‚
The select_next_node will route to the human node if the flag is set.
`select_next_node` ã¯ã€ãƒ•ãƒ©ã‚°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã«humanãƒãƒ¼ãƒ‰ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã—ã¾ã™ã€‚
Otherwise, it lets the prebuilt tools_condition function choose the next node.
ãã†ã§ãªã‘ã‚Œã°ã€prebuilt tools_conditioné–¢æ•°ãŒæ¬¡ã®ãƒãƒ¼ãƒ‰ã‚’é¸æŠã—ã¾ã™ã€‚
Recall that the tools_condition function simply checks to see if the chatbot has responded with any tool_calls in its response message.
tools_conditioné–¢æ•°ã¯ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãŒå¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«ã‚’å«ã‚ã¦å¿œç­”ã—ãŸã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚
If so, it routes to the action node.
ãã†ã§ã‚ã‚Œã°ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¼ãƒ‰ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã—ã¾ã™ã€‚
Otherwise, it ends the graph.
ãã†ã§ãªã‘ã‚Œã°ã€ã‚°ãƒ©ãƒ•ã¯çµ‚äº†ã—ã¾ã™ã€‚

```python
def select_next_node(state: State):
    if state["ask_human"]:
        return "human"
    # Otherwise, we can route as before
    return tools_condition(state)

graph_builder.add_conditional_edges("chatbot", select_next_node, {"human": "human", "tools": "tools", END: END},)
```

Finally, add the simple directed edges and compile the graph.
æœ€å¾Œã«ã€ã‚·ãƒ³ãƒ—ãƒ«ãªæœ‰å‘ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ ã—ã€ã‚°ãƒ©ãƒ•ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚
These edges instruct the graph to always flow from node a -> b whenever a finishes executing.
ã“ã‚Œã‚‰ã®ã‚¨ãƒƒã‚¸ã¯ã€ãƒãƒ¼ãƒ‰aãŒå®Ÿè¡Œã‚’çµ‚äº†ã™ã‚‹ãŸã³ã«å¸¸ã«ãƒãƒ¼ãƒ‰aã‹ã‚‰ãƒãƒ¼ãƒ‰bã«æµã‚Œã‚‹ã‚ˆã†ã«ã‚°ãƒ©ãƒ•ã«æŒ‡ç¤ºã—ã¾ã™ã€‚

```python

# The rest is the same

graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge("human", "chatbot")
graph_builder.add_edge(START, "chatbot")

memory = MemorySaver()
graph = graph_builder.compile(checkpointer=memory,
    # We interrupt before 'human' here instead.
    interrupt_before=["human"],
)

```

If you have the visualization dependencies installed, you can see the graph structure below:
è¦–è¦šåŒ–ä¾å­˜é–¢ä¿‚ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆã€ä»¥ä¸‹ã«ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’è¡¨ç¤ºã§ãã¾ã™ã€‚

```python
from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass
```

The chat bot can either request help from a human (chatbot->select->human), invoke the search engine tool (chatbot->select->action), or directly respond (chatbot->select->end).
ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯ã€äººé–“ã‹ã‚‰ã®æ”¯æ´ã‚’è¦æ±‚ã™ã‚‹ã“ã¨ã‚‚ï¼ˆchatbot->select->humanï¼‰ã€æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™ã“ã¨ã‚‚ï¼ˆchatbot->select->actionï¼‰ã€ç›´æ¥å¿œç­”ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼ˆchatbot->select->endï¼‰ã€‚
Once an action or request has been made, the graph will transition back to the chatbot node to continue operations.
ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¾ãŸã¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒè¡Œã‚ã‚Œã‚‹ã¨ã€ã‚°ãƒ©ãƒ•ã¯ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒãƒ¼ãƒ‰ã«æˆ»ã‚Šã€æ“ä½œã‚’ç¶šè¡Œã—ã¾ã™ã€‚
Let's see this graph in action.
ã“ã®ã‚°ãƒ©ãƒ•ã‚’å®Ÿéš›ã«è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
We will request for expert assistance to illustrate our graph.
ã‚°ãƒ©ãƒ•ã‚’ç¤ºã™ãŸã‚ã«å°‚é–€å®¶ã®æ”¯æ´ã‚’è¦æ±‚ã—ã¾ã™ã€‚

```python
user_input = "I need some expert guidance for building this AI agent. Could you request assistance for me?"
config = {"configurable": {"thread_id": "1"}}

# The config is the **second positional argument** to stream() or invoke()

events = graph.stream({"messages": [("user", user_input)]}, config, stream_mode="values")
for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()
```

```shell
================================[1m Human Message [0m=================================
I need some expert guidance for building this AI agent. Could you request assistance for me?
==================================[1m Ai Message [0m==================================
[{'text': "Certainly! I understand that you need expert guidance for building an AI agent. I'll use the RequestAssistance function to escalate your request to an expert who can provide you with the specialized knowledge and support you need. Let me do that for you right away.", 'type': 'text'}, {'id': 'toolu_01Mo3N2c1byuSZwT1vyJWRia', 'input': {'request': 'The user needs expert guidance for building an AI agent. They require specialized knowledge and support in AI development and implementation.'}, 'name': 'RequestAssistance', 'type': 'tool_use'}]
Tool Calls: RequestAssistance (toolu_01Mo3N2c1byuSZwT1vyJWRia)
Call ID: toolu_01Mo3N2c1byuSZwT1vyJWRia
Args: request: The user needs expert guidance for building an AI agent. They require specialized knowledge and support in AI development and implementation.
```

```python
snapshot = graph.get_state(config)
snapshot.next
```

```

('human',)

```

The graph state is indeed interrupted before the 'human' node.
ã‚°ãƒ©ãƒ•çŠ¶æ…‹ã¯ç¢ºã‹ã«ã€Œäººé–“ã€ãƒãƒ¼ãƒ‰ã®å‰ã§ä¸­æ–­ã•ã‚Œã¦ã„ã¾ã™ã€‚
We can act as the "expert" in this scenario and manually update the state by adding a new ToolMessage with our input.
ã“ã®ã‚·ãƒŠãƒªã‚ªã§ã¯ã€Œå°‚é–€å®¶ã€ã¨ã—ã¦è¡Œå‹•ã—ã€ç§ãŸã¡ã®å…¥åŠ›ã‚’å«ã‚€æ–°ã—ã„ToolMessageã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§çŠ¶æ…‹ã‚’æ‰‹å‹•ã§æ›´æ–°ã§ãã¾ã™ã€‚
Next, respond to the chatbot's request by:
æ¬¡ã«ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¿œã˜ã¦ï¼š

1. Creating a ToolMessage with our response.
1. ç§ãŸã¡ã®å¿œç­”ã‚’å«ã‚€ToolMessageã‚’ä½œæˆã—ã¾ã™ã€‚
2. Calling update_state to manually update the graph state.
2. update_stateã‚’å‘¼ã³å‡ºã—ã¦ã€ã‚°ãƒ©ãƒ•çŠ¶æ…‹ã‚’æ‰‹å‹•ã§æ›´æ–°ã—ã¾ã™ã€‚

```

ai_message = snapshot.values["messages"][-1]
human_response = ("We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. "
                  "It's much more reliable and extensible than simple autonomous agents.")
tool_message = create_response(human_response, ai_message)
graph.update_state(config, {"messages": [tool_message]})

```

```

{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7d092-bb30-6bee-8002-015e7e1c56c0'}}

```

You can inspect the state to confirm our response was added.
çŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ã€ç§ãŸã¡ã®å¿œç­”ãŒè¿½åŠ ã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèªã§ãã¾ã™ã€‚

```

graph.get_state(config).values["messages"]

```

```

[HumanMessage(content='I need some expert guidance for building this AI agent. Could you request assistance for me?', additional_kwargs={}, response_metadata={}, id='3f28f959-9ab7-489a-9c58-7ed1b49cedf3'),
 AIMessage(content=[{'text': "Certainly! I understand that you need expert guidance for building an AI agent. I'll use the RequestAssistance function to escalate your request to an expert who can provide you with the specialized knowledge and support you need. Let me do that for you right away.", 'type': 'text'}, {'id': 'toolu_01Mo3N2c1byuSZwT1vyJWRia', 'input': {'request': 'The user needs expert guidance for building an AI agent. They require specialized knowledge and support in AI development and implementation.'}, 'name': 'RequestAssistance', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01VRnZvVbgsVRbQaQuvsziDx', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 516, 'output_tokens': 130}}, id='run-4e3f7906-5887-40d9-9267-5beefe7b3b76-0', tool_calls=[{'name': 'RequestAssistance', 'args': {'request': 'The user needs expert guidance for building an AI agent. They require specialized knowledge and support in AI development and implementation.'}, 'id': 'toolu_01Mo3N2c1byuSZwT1vyJWRia', 'type': 'tool_call'}], usage_metadata={'input_tokens': 516, 'output_tokens': 130, 'total_tokens': 646}),
 ToolMessage(content="We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents.", id='8583b899-d898-4051-9f36-f5e5d11e9a37', tool_call_id='toolu_01Mo3N2c1byuSZwT1vyJWRia')]

```

Next, resume the graph by invoking it with None as the inputs.
æ¬¡ã«ã€Noneã‚’å…¥åŠ›ã¨ã—ã¦å‘¼ã³å‡ºã™ã“ã¨ã§ã‚°ãƒ©ãƒ•ã‚’å†é–‹ã—ã¾ã™ã€‚

```

events = graph.stream(None, config, stream_mode="values")
for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()

```

```

=================================[1m Tool Message [0m=================================
We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents.
=================================[1m Tool Message [0m=================================
We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents.
==================================[1m Ai Message [0m==================================
Thank you for your patience. I've escalated your request to our expert team, and they have provided some initial guidance. Here's what they suggest:
The experts recommend that you check out LangGraph for building your AI agent. They mention that LangGraph is a more reliable and extensible option compared to simple autonomous agents.
LangGraph is likely a framework or tool designed specifically for creating complex AI agents. It seems to offer advantages in terms of reliability and extensibility, which are crucial factors when developing sophisticated AI systems.
To further assist you, I can provide some additional context and next steps:

1. Research LangGraph: Look up documentation, tutorials, and examples of LangGraph to understand its features and how it can help you build your AI agent.
2. Compare with other options: While the experts recommend LangGraph, it might be useful to understand how it compares to other AI agent development frameworks or tools you might have been considering.
3. Assess your requirements: Consider your specific needs for the AI agent you want to build. Think about the tasks it needs to perform, the level of complexity required, and how LangGraph's features align with these requirements.
4. Start with a small project: If you decide to use LangGraph, consider beginning with a small, manageable project to familiarize yourself with the framework.
5. Seek community support: Look for LangGraph user communities, forums, or discussion groups where you can ask questions and get additional support as you build your agent.
6. Consider additional training: Depending on your current skill level, you might want to look into courses or workshops that focus on AI agent development, particularly those that cover LangGraph.
Do you have any specific questions about LangGraph or AI agent development that you'd like me to try to answer? Or would you like me to search for more detailed information about LangGraph and its features?

```

Congratulations!
ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼
You've now added an additional node to your assistant graph to let the chat bot decide for itself whether or not it needs to interrupt execution.
ã“ã‚Œã§ã€**ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãŒå®Ÿè¡Œã‚’ä¸­æ–­ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’è‡ªåˆ†ã§æ±ºå®šã§ãã‚‹ã‚ˆã†ã«ã€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚°ãƒ©ãƒ•ã«è¿½åŠ ã®ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ **ã—ã¾ã—ãŸã€‚
You did so by updating the graph State with a new ask_human field and modifying the interruption logic when compiling the graph.
ã‚°ãƒ©ãƒ•çŠ¶æ…‹ã‚’æ–°ã—ã„ask_humanãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§æ›´æ–°ã—ã€ã‚°ãƒ©ãƒ•ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹éš›ã«ä¸­æ–­ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã§å®Ÿç¾ã—ã¾ã—ãŸã€‚
This lets you dynamically include a human in the loop while maintaining full memory every time you execute the graph.
ã“ã‚Œã«ã‚ˆã‚Šã€ã‚°ãƒ©ãƒ•ã‚’å®Ÿè¡Œã™ã‚‹ãŸã³ã«å®Œå…¨ãªãƒ¡ãƒ¢ãƒªã‚’ç¶­æŒã—ãªãŒã‚‰ã€å‹•çš„ã«äººé–“ã‚’ãƒ«ãƒ¼ãƒ—ã«å«ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
We're almost done with the tutorial, but there is one more concept we'd like to review before finishing that connects checkpointing and state updates.
ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¯ã»ã¼å®Œäº†ã§ã™ãŒã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨çŠ¶æ…‹æ›´æ–°ã‚’æ¥ç¶šã™ã‚‹å‰ã«ç¢ºèªã—ãŸã„ã‚‚ã†ä¸€ã¤ã®æ¦‚å¿µãŒã‚ã‚Šã¾ã™ã€‚
This section's code is reproduced below for your reference.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚³ãƒ¼ãƒ‰ã¯ã€å‚è€ƒã®ãŸã‚ã«ä»¥ä¸‹ã«å†æ²ã—ã¾ã™ã€‚

```python
from typing import Annotated
from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import BaseMessage

# NOTE: you must use langchain-core >= 0.3 with Pydantic v2

from pydantic import BaseModel
from typing_extensions import TypedDict
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

class State(TypedDict):
    messages: Annotated[list, add_messages]  # This flag is new
    ask_human: bool

class RequestAssistance(BaseModel):
    """Escalate the conversation to an expert.
    ä¼šè©±ã‚’å°‚é–€å®¶ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¾ã™ã€‚
    Use this if you are unable to assist directly or if the user requires support beyond your permissions.
    ç›´æ¥æ”¯æ´ã§ããªã„å ´åˆã‚„ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚ãªãŸã®æ¨©é™ã‚’è¶…ãˆãŸæ”¯æ´ã‚’å¿…è¦ã¨ã™ã‚‹å ´åˆã«ä½¿ç”¨ã—ã¾ã™ã€‚
    To use this function, relay the user's 'request' so the expert can provide the right guidance.
    ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œãƒªã‚¯ã‚¨ã‚¹ãƒˆã€ã‚’ä¸­ç¶™ã—ã¦ã€å°‚é–€å®¶ãŒé©åˆ‡ãªã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’æä¾›ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
    """
    request: str

tool = TavilySearchResults(max_results=2)
tools = [tool]
llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")

# We can bind the llm to a tool definition, a pydantic model, or a json schema

llm_with_tools = llm.bind_tools(tools + [RequestAssistance])

def chatbot(state: State):
    response = llm_with_tools.invoke(state["messages"])
    ask_human = False
    if (response.tool_calls and response.tool_calls[0]["name"] == RequestAssistance.**name**):
        ask_human = True
    return {
        "messages": [response],
        "ask_human": ask_human
    }

graph_builder = StateGraph(State)
graph_builder.add_node("chatbot", chatbot)
graph_builder.add_node("tools", ToolNode(tools=[tool]))

def create_response(response: str, ai_message: AIMessage):
    return ToolMessage(content=response, tool_call_id=ai_message.tool_calls[0]["id"],)

def human_node(state: State):
    new_messages = []
    if not isinstance(state["messages"][-1], ToolMessage):
        # Typically, the user will have updated the state during the interrupt.
        # If they choose not to, we will include a placeholder ToolMessage to
        # let the LLM continue.
        new_messages.append(create_response("No response from human.", state["messages"][-1]))
    return {
        # Append the new messages
        "messages": new_messages,
        # Unset the flag
        "ask_human": False,
    }

graph_builder.add_node("human", human_node)

def select_next_node(state: State):
    if state["ask_human"]:
        return "human"
    # Otherwise, we can route as before
    return tools_condition(state)

graph_builder.add_conditional_edges("chatbot", select_next_node, {"human": "human", "tools": "tools", "**end**": "**end**"},)
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge("human", "chatbot")
graph_builder.set_entry_point("chatbot")

memory = MemorySaver()
graph = graph_builder.compile(checkpointer=memory, interrupt_before=["human"],)
```

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## Part 7: Time TravelÂ¶ ç¬¬7éƒ¨: ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒ™ãƒ«

In a typical chat bot workflow, the user interacts with the bot 1 or more times to accomplish a task.
å…¸å‹çš„ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã¯ã€**ãƒ¦ãƒ¼ã‚¶ã¯ã‚¿ã‚¹ã‚¯ã‚’é”æˆã™ã‚‹ãŸã‚ã«ãƒœãƒƒãƒˆã¨1å›ä»¥ä¸Šå¯¾è©±**ã—ã¾ã™ã€‚
In the previous sections, we saw how to add memory and a human-in-the-loop to be able to checkpoint our graph state and manually override the state to control future responses.
å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒ¡ãƒ¢ãƒªã‚’è¿½åŠ ã—ã€ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ãƒ»ã‚¤ãƒ³ãƒ»ã‚¶ãƒ»ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨ã—ã¦ã‚°ãƒ©ãƒ•ã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã—ã€å°†æ¥ã®å¿œç­”ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã«çŠ¶æ…‹ã‚’æ‰‹å‹•ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã™ã‚‹æ–¹æ³•ã‚’è¦‹ã¾ã—ãŸã€‚

But what if you want to let your user start from a previous response and "branch off" to explore a separate outcome?
ã—ã‹ã—ã€ãƒ¦ãƒ¼ã‚¶ãŒä»¥å‰ã®å¿œç­”ã‹ã‚‰å§‹ã‚ã¦ã€Œåˆ†å²ã€ã—ã€åˆ¥ã®çµæœã‚’æ¢æ±‚ã§ãã‚‹ã‚ˆã†ã«ã—ãŸã„å ´åˆã¯ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿ
Or what if you want users to be able to "rewind" your assistant's work to fix some mistakes or try a different strategy (common in applications like autonomous software engineers)?
ã‚ã‚‹ã„ã¯ã€**ãƒ¦ãƒ¼ã‚¶ãŒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ä½œæ¥­ã‚’ã€Œå·»ãæˆ»ã—ã¦ã€ã€ã„ãã¤ã‹ã®é–“é•ã„ã‚’ä¿®æ­£ã—ãŸã‚Šã€ç•°ãªã‚‹æˆ¦ç•¥ã‚’è©¦ã—ãŸã‚Šã§ãã‚‹ã‚ˆã†ã«ã—ãŸã„**å ´åˆï¼ˆè‡ªå¾‹ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®ã‚ˆã†ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ä¸€èˆ¬çš„ï¼‰ã«ã¯ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿ

You can create both of these experiences and more using LangGraph's built-in "time travel" functionality.
**LangGraphã®çµ„ã¿è¾¼ã¿ã€Œã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒ™ãƒ«ã€æ©Ÿèƒ½**ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã“ã‚Œã‚‰ã®ä½“é¨“ã‚„ãã®ä»–ã®ä½“é¨“ã‚’ä½œæˆã§ãã¾ã™ã€‚

In this section, you will "rewind" your graph by fetching a checkpoint using the graph's get_state_history method.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã‚°ãƒ©ãƒ•ã® `get_state_history` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—ã™ã‚‹ã“ã¨ã§ã€ã‚°ãƒ©ãƒ•ã‚’ã€Œå·»ãæˆ»ã—ã¾ã™ã€ã€‚

You can then resume execution at this previous point in time.
ãã®å¾Œã€ã“ã®ä»¥å‰ã®æ™‚ç‚¹ã§å®Ÿè¡Œã‚’å†é–‹ã§ãã¾ã™ã€‚

First, recall our chatbot graph.
ã¾ãšã€ç§ãŸã¡ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚°ãƒ©ãƒ•ã‚’æ€ã„å‡ºã—ã¦ãã ã•ã„ã€‚

We don't need to make any changes from before:
ä»¥å‰ã‹ã‚‰å¤‰æ›´ã‚’åŠ ãˆã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ï¼š

```python
from typing import Annotated, Literal
from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import AIMessage, ToolMessage

# NOTE: you must use langchain-core >= 0.3 with Pydantic v2

from pydantic import BaseModel
from typing_extensions import TypedDict
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

class State(TypedDict):
    messages: Annotated[list, add_messages]
    # This flag is new
    ask_human: bool

class RequestAssistance(BaseModel):
    """Escalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.
    To use this function, relay the user's 'request' so the expert can provide the right guidance."""
    request: str

tool = TavilySearchResults(max_results=2)
tools = [tool]
llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")

# We can bind the llm to a tool definition, a pydantic model, or a json schema

llm_with_tools = llm.bind_tools(tools + [RequestAssistance])

def chatbot(state: State):
    response = llm_with_tools.invoke(state["messages"])
    ask_human = False
    if response.tool_calls and response.tool_calls[0]["name"] == RequestAssistance.**name**:
        ask_human = True
    return {"messages": [response], "ask_human": ask_human}

graph_builder = StateGraph(State)
graph_builder.add_node("chatbot", chatbot)
graph_builder.add_node("tools", ToolNode(tools=[tool]))

def create_response(response: str, ai_message: AIMessage):
    return ToolMessage(content=response, tool_call_id=ai_message.tool_calls[0]["id"],)

def human_node(state: State):
    new_messages = []
    if not isinstance(state["messages"][-1], ToolMessage):
        # Typically, the user will have updated the state during the interrupt.
        # If they choose not to, we will include a placeholder ToolMessage to
        # let the LLM continue.
        new_messages.append(create_response("No response from human.", state["messages"][-1]))
    return {
        # Append the new messages
        "messages": new_messages,
        # Unset the flag
        "ask_human": False,
    }

graph_builder.add_node("human", human_node)

def select_next_node(state: State):
    if state["ask_human"]:
        return "human"
    # Otherwise, we can route as before
    return tools_condition(state)

graph_builder.add_conditional_edges("chatbot", select_next_node, {"human": "human", "tools": "tools", END: END},)
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge("human", "chatbot")
graph_builder.add_edge(START, "chatbot")

memory = MemorySaver()
graph = graph_builder.compile(checkpointer=memory, interrupt_before=["human"],)

```

```

from IPython.display import Image, display
try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass

```

Let's have our graph take a couple steps.
ã‚°ãƒ©ãƒ•ã«ã„ãã¤ã‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã•ã›ã¾ã—ã‚‡ã†ã€‚

Every step will be checkpointed in its state history:
ã™ã¹ã¦ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€ãã®çŠ¶æ…‹å±¥æ­´ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã•ã‚Œã¾ã™ã€‚

```python
config = {"configurable": {"thread_id": "1"}}
events = graph.stream({"messages": [("user", "I'm learning LangGraph. Could you do some research on it for me?")]}, config, stream_mode="values",)
for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()
```

```shell
================================[1m Human Message [0m=================================
I'm learning LangGraph. Could you do some research on it for me?
==================================[1m Ai Message [0m==================================
[{'text': "Certainly! I'd be happy to research LangGraph for you. To get the most up-to-date and accurate information, I'll use the Tavily search function to gather details about LangGraph. Let me do that for you now.", 'type': 'text'}, {'id': 'toolu_019HPZEw6v1eSLBXnwxk6MZm', 'input': {'query': 'LangGraph framework for language models'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls: tavily_search_results_json (toolu_019HPZEw6v1eSLBXnwxk6MZm)
Call ID: toolu_019HPZEw6v1eSLBXnwxk6MZm
Args: query: LangGraph framework for language models
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json
[{"url": "https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141", "content": "LangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or ..."}, {"url": "https://towardsdatascience.com/from-basics-to-advanced-exploring-langgraph-e8c1cf4db787", "content": "LangChain is one of the leading frameworks for building applications powered by Lardge Language Models. With the LangChain Expression Language (LCEL), defining and executing step-by-step action sequences â€” also known as chains â€” becomes much simpler. In more technical terms, LangChain allows us to create DAGs (directed acyclic graphs)."}]
==================================[1m Ai Message [0m==================================
Thank you for your patience. I've gathered some information about LangGraph for you. Let me summarize the key points:

1. What is LangGraph?
LangGraph is a powerful tool designed for building stateful, multi-actor applications using Large Language Models (LLMs). It's an extension of the LangChain library, which is already a popular framework for developing LLM-powered applications.
2. Purpose and Functionality:

- LangGraph allows developers to coordinate multiple chains or actors within a single application.
- It enhances the capabilities of LangChain by introducing more complex, stateful workflows.

3. Relation to LangChain:

- LangGraph builds upon LangChain, which is one of the leading frameworks for creating LLM-powered applications.
- LangChain itself uses the LangChain Expression Language (LCEL) to define and execute step-by-step action sequences, also known as chains.
- LangChain allows the creation of DAGs (Directed Acyclic Graphs), which represent the flow of operations in an application.

4. Key Features:

- Stateful Applications: Unlike simple query-response models, LangGraph allows the creation of applications that maintain state across interactions.
- Multi-Actor Systems: It supports coordinating multiple AI "actors" or components within a single application, enabling more complex interactions and workflows.

5. Use Cases:
While not explicitly mentioned in the search results, LangGraph is typically used for creating more sophisticated AI applications such as:

- Multi-turn conversational agents
- Complex task-planning systems
- Applications requiring memory and context management across multiple steps or actors
Learning LangGraph can be a valuable skill, especially if you're interested in developing advanced applications with LLMs that go beyond simple question-answering or text generation tasks. It allows for the creation of more dynamic, interactive, and stateful AI systems.
Is there any specific aspect of LangGraph you'd like to know more about, or do you have any questions about how it compares to or works with LangChain?
```

```python
# events = graph.stream({"messages": [("user", "Ya that's helpful. Maybe I'll build an autonomous agent with it!")]}, config, stream_mode="values",)
events = graph.stream({"messages": [("user", "ã‚ã‚ã€ã¨ã¦ã‚‚åŠ©ã‹ã‚Šã¾ã™! ãã‚Œã§è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“!")]}, config, stream_mode="values",)
for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()
```

```shell
================================[1m Human Message [0m=================================
Ya that's helpful. Maybe I'll build an autonomous agent with it!
==================================[1m Ai Message [0m==================================
[{'text': "That's an excellent idea! Building an autonomous agent with LangGraph is a great way to explore its capabilities and learn about advanced AI application development. LangGraph's features make it well-suited for creating autonomous agents. Let me provide some additional insights and encouragement for your project.", 'type': 'text'}, {'id': 'toolu_017t6BS5rNCzFWcpxRizDKjE', 'input': {'query': 'building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls: tavily_search_results_json (toolu_017t6BS5rNCzFWcpxRizDKjE)
Call ID: toolu_017t6BS5rNCzFWcpxRizDKjE
Args: query: building autonomous agents with LangGraph examples and tutorials
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json
[{"url": "https://medium.com/@lucas.dahan/hands-on-langgraph-building-a-multi-agent-assistant-06aa68ed942f", "content": "Building the Graph. With our agents defined, we'll create a graph.py file to orchestrate their interactions. The basic graph structure in LangGraph is really simple, here we are going to use ..."}, {"url": "https://medium.com/@cplog/building-tool-calling-conversational-ai-with-langchain-and-langgraph-a-beginners-guide-8d6986cc589e", "content": "Introduction to AI Agent with LangChain and LangGraph: A Beginnerâ€™s Guide Two powerful tools revolutionizing this field are LangChain and LangGraph. In this guide, weâ€™ll explore how these technologies can be combined to build a sophisticated AI assistant capable of handling complex conversations and tasks. Tool calling is a standout feature in agentic design, allowing the LLM to interact with external systems or perform specific tasks via the @tool decorator. While the Assistant class presented here is one approach, the flexibility of tool calling and LangGraph allows for a wide range of designs. With LangChain and LangGraph, you can build a powerful, flexible AI assistant capable of handling complex tasks and conversations. Tool calling significantly enhances the AIâ€™s capabilities by enabling interaction with external systems."}]
==================================[1m Ai Message [0m==================================
Your enthusiasm for building an autonomous agent with LangGraph is fantastic! This project will not only help you learn more about LangGraph but also give you hands-on experience with cutting-edge AI development. Here are some insights and tips to get you started:

1. Multi-Agent Systems:
LangGraph excels at creating multi-agent systems. You could design your autonomous agent as a collection of specialized sub-agents, each handling different aspects of tasks or knowledge domains.
2. Graph Structure:
The basic graph structure in LangGraph is straightforward. You'll create a graph.py file to orchestrate the interactions between your agents or components.
3. Tool Calling:
A key feature you can incorporate is tool calling. This allows your LLM-based agent to interact with external systems or perform specific tasks. You can implement this using the @tool decorator in your code.
4. Flexibility in Design:
LangGraph offers great flexibility in designing your agent. While there are example structures like the Assistant class, you have the freedom to create a wide range of designs tailored to your specific needs.
5. Complex Conversations and Tasks:
Your autonomous agent can be designed to handle sophisticated conversations and complex tasks. This is where LangGraph's stateful nature really shines, allowing your agent to maintain context over extended interactions.
6. Integration with LangChain:
Since LangGraph builds upon LangChain, you can leverage features from both. This combination allows for powerful, flexible AI assistants capable of managing intricate workflows.
7. External System Interaction:
Consider incorporating external APIs or databases to enhance your agent's capabilities. This could include accessing real-time data, performing calculations, or interacting with other services.
8. Tutorial Resources:
There are tutorials available that walk through the process of building AI assistants with LangChain and LangGraph. These can be excellent starting points for your project.
To get started, you might want to:
1. Set up your development environment with LangChain and LangGraph.
2. Define the core functionalities you want your autonomous agent to have.
3. Design the overall structure of your agent, possibly as a multi-agent system.
4. Implement basic interactions and gradually add more complex features like tool calling and state management.
5. Test your agent thoroughly with various scenarios to ensure robust performance.
Remember, building an autonomous agent is an iterative process. Start with a basic version and progressively enhance its capabilities. This approach will help you understand the intricacies of LangGraph while creating a sophisticated AI application.
Do you have any specific ideas about what kind of tasks or domain you want your autonomous agent to specialize in? This could help guide the design and implementation process.
```

Now that we've had the agent take a couple steps, we can replay the full state history to see everything that occurred.
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã„ãã¤ã‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¸ã‚“ã ã®ã§ã€å®Œå…¨ãªçŠ¶æ…‹å±¥æ­´ã‚’å†ç”Ÿã—ã¦ç™ºç”Ÿã—ãŸã™ã¹ã¦ã‚’ç¢ºèªã§ãã¾ã™ã€‚

```python
to_replay = None
for state in graph.get_state_history(config):
    print("Num Messages: ", len(state.values["messages"]), "Next: ", state.next)
    print("-" * 80)
    if len(state.values["messages"]) == 6:
        # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.
        to_replay = state
```

```shell
Num Messages:  8 Next:  ()
--------------------------------------------------------------------------------

Num Messages:  7 Next:  ('chatbot',)
--------------------------------------------------------------------------------

Num Messages:  6 Next:  ('tools',)
--------------------------------------------------------------------------------

Num Messages:  5 Next:  ('chatbot',)
--------------------------------------------------------------------------------

Num Messages:  4 Next:  ('**start**',)
--------------------------------------------------------------------------------

Num Messages:  4 Next:  ()
--------------------------------------------------------------------------------

Num Messages:  3 Next:  ('chatbot',)
--------------------------------------------------------------------------------

Num Messages:  2 Next:  ('tools',)
--------------------------------------------------------------------------------

Num Messages:  1 Next:  ('chatbot',)
--------------------------------------------------------------------------------

Num Messages:  0 Next:  ('**start**',)
--------------------------------------------------------------------------------
```

Notice that checkpoints are saved for every step of the graph. This spans invocations so you can rewind across a full thread's history. We've picked out to_replay as a state to resume from. This is the state after the chatbot node in the second graph invocation above.
**ã‚°ãƒ©ãƒ•ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒä¿å­˜ã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„**ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã¯å‘¼ã³å‡ºã—ã‚’ã¾ãŸã„ã§ã„ã‚‹ãŸã‚ã€ã‚¹ãƒ¬ãƒƒãƒ‰å…¨ä½“ã®å±¥æ­´ã‚’å·»ãæˆ»ã™ã“ã¨ãŒã§ãã¾ã™ã€‚ä¸Šè¨˜ã®2ç•ªç›®ã®ã‚°ãƒ©ãƒ•å‘¼ã³å‡ºã—ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒãƒ¼ãƒ‰ã®å¾Œã®çŠ¶æ…‹ã¨ã—ã¦å†é–‹ã™ã‚‹ãŸã‚ã« to_replay ã‚’é¸æŠã—ã¾ã—ãŸã€‚

Resuming from this point should call the action node next.
ã“ã®ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã™ã‚‹ã¨ã€æ¬¡ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¼ãƒ‰ãŒå‘¼ã³å‡ºã•ã‚Œã‚‹ã¯ãšã§ã™ã€‚

```python
print(to_replay.next)
print(to_replay.config)
```

```shell
('tools',){'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7d094-2634-687c-8006-49ddde5b2f1c'}}
```

Notice that the checkpoint's config (to_replay.config) contains a checkpoint_id timestamp. Providing this checkpoint_id value tells LangGraph's checkpointer to load the state from that moment in time. Let's try it below:
ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®æ§‹æˆï¼ˆto_replay.configï¼‰ã«ã¯ã€**ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆIDã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„**ã—ã¦ãã ã•ã„ã€‚ã“ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆIDå€¤ã‚’æä¾›ã™ã‚‹ã¨ã€LangGraphã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ã‚¿ãƒ¼ã¯ãã®æ™‚ç‚¹ã®çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã‚€ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚ä»¥ä¸‹ã§è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
# The `checkpoint_id` in the `to_replay.config` corresponds to a state we've persisted to our checkpointer

for event in graph.stream(None, to_replay.config, stream_mode="values"):
    if "messages" in event:
        event["messages"][-1].pretty_print()

```

```

==================================[1m Ai Message [0m==================================
[{'text': "That's an excellent idea! Building an autonomous agent with LangGraph is a great way to explore its capabilities and learn about advanced AI application development. LangGraph's features make it well-suited for creating autonomous agents. Let me provide some additional insights and encouragement for your project.", 'type': 'text'}, {'id': 'toolu_017t6BS5rNCzFWcpxRizDKjE', 'input': {'query': 'building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls: tavily_search_results_json (toolu_017t6BS5rNCzFWcpxRizDKjE)
Call ID: toolu_017t6BS5rNCzFWcpxRizDKjE
Args: query: building autonomous agents with LangGraph examples and tutorials
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json
[{"url": "https://blog.langchain.dev/how-to-build-the-ultimate-ai-automation-with-multi-agent-collaboration/", "content": "Learn how to create an autonomous research assistant using LangGraph, an extension of LangChain for agent and multi-agent flows. Follow the steps to define the graph state, initialize the graph, and run the agents for planning, research, review, writing and publishing."}, {"url": "https://medium.com/@lucas.dahan/hands-on-langgraph-building-a-multi-agent-assistant-06aa68ed942f", "content": "Building the Graph. With our agents defined, we'll create a graph.py file to orchestrate their interactions. The basic graph structure in LangGraph is really simple, here we are going to use ..."}]
==================================[1m Ai Message [0m==================================
Great choice! Building an autonomous agent with LangGraph is an excellent way to dive deep into its capabilities. Based on the additional information I've found, here are some insights and steps to help you get started:

1. LangGraph for Autonomous Agents:
LangGraph is particularly well-suited for creating autonomous agents, especially those involving multi-agent collaboration. It allows you to create complex, stateful workflows that can simulate autonomous behavior.
2. Example Project: Autonomous Research Assistant
One popular example is building an autonomous research assistant. This type of project can help you understand the core concepts of LangGraph while creating something useful.
3. Key Steps in Building an Autonomous Agent:
a. Define the Graph State: This involves setting up the structure that will hold the agent's state and context.
b. Initialize the Graph: Set up the initial conditions and parameters for your agent.
c. Create Multiple Agents: For a complex system, you might create several specialized agents, each with a specific role (e.g., planning, research, review, writing).
d. Orchestrate Interactions: Use LangGraph to manage how these agents interact and collaborate.
4. Components of an Autonomous Agent:

- Planning Agent: Determines the overall strategy and steps.
- Research Agent: Gathers necessary information.
- Review Agent: Evaluates and refines the work.
- Writing Agent: Produces the final output.
- Publishing Agent: Handles the final distribution or application of results.

5. Implementation Tips:

- Start with a simple graph structure in LangGraph.
- Define clear roles and responsibilities for each agent or component.
- Use LangGraph's features to manage state and context across the different stages of your agent's workflow.

6. Learning Resources:

- Look for tutorials and examples specifically on building multi-agent systems with LangGraph.
- The LangChain documentation and community forums can be valuable resources, as LangGraph builds upon LangChain.

7. Potential Applications:

- Autonomous research assistants
- Complex task automation systems
- Interactive storytelling agents
- Autonomous problem-solving systems
Building an autonomous agent with LangGraph is an exciting project that will give you hands-on experience with advanced concepts in AI application development. It's a great way to learn about state management, multi-agent coordination, and complex workflow design in AI systems.
As you embark on this project, remember to start small and gradually increase complexity. You might begin with a simple autonomous agent that performs a specific task, then expand its capabilities and add more agents or components as you become more comfortable with LangGraph.
Do you have a specific type of autonomous agent in mind, or would you like some suggestions for beginner-friendly autonomous agent projects to start with?
```

Congratulations! You've now used time-travel checkpoint traversal in LangGraph.
ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼ã“ã‚Œã§ã€LangGraphã«ãŠã‘ã‚‹ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒ™ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒˆãƒ©ãƒãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚

Being able to rewind and explore alternative paths opens up a world of possibilities for debugging, experimentation, and interactive applications.
**å·»ãæˆ»ã—ã¦ä»£æ›¿ã®çµŒè·¯ã‚’æ¢æ±‚ã§ãã‚‹ã“ã¨**ã¯ã€ãƒ‡ãƒãƒƒã‚°ã€å®Ÿé¨“ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å¯èƒ½æ€§ã®ä¸–ç•Œã‚’é–‹ãã¾ã™ã€‚

## Next Steps æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

Take your journey further by exploring deployment and advanced features:
ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚„é«˜åº¦ãªæ©Ÿèƒ½ã‚’æ¢æ±‚ã™ã‚‹ã“ã¨ã§ã€ã‚ãªãŸã®æ—…ã‚’ã•ã‚‰ã«é€²ã‚ã¦ãã ã•ã„ï¼š

### Server Quickstart ã‚µãƒ¼ãƒãƒ¼ã®ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

- LangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.
- LangGraphã‚µãƒ¼ãƒãƒ¼ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼šLangGraphã‚µãƒ¼ãƒãƒ¼ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§èµ·å‹•ã—ã€REST APIã¨LangGraph Studio Web UIã‚’ä½¿ç”¨ã—ã¦å¯¾è©±ã—ã¾ã™ã€‚

### LangGraph Cloud

- LangGraph Cloud QuickStart: Deploy your LangGraph app using LangGraph Cloud.
- LangGraph Cloudã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼šLangGraph Cloudã‚’ä½¿ç”¨ã—ã¦LangGraphã‚¢ãƒ—ãƒªã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã™ã€‚

### LangGraph FrameworkÂ¶ LangGraphãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

- LangGraph Concepts: Learn the foundational concepts of LangGraph.
- LangGraphã®æ¦‚å¿µï¼šLangGraphã®åŸºæœ¬çš„ãªæ¦‚å¿µã‚’å­¦ã³ã¾ã™ã€‚
- LangGraph How-to Guides: Guides for common tasks with LangGraph.
- LangGraph How-toã‚¬ã‚¤ãƒ‰ï¼šLangGraphã§ã®ä¸€èˆ¬çš„ãªã‚¿ã‚¹ã‚¯ã®ã‚¬ã‚¤ãƒ‰ã€‚

### LangGraph PlatformÂ¶ LangGraphãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 

Expand your knowledge with these resources:
ã“ã‚Œã‚‰ã®ãƒªã‚½ãƒ¼ã‚¹ã§çŸ¥è­˜ã‚’åºƒã’ã¾ã—ã‚‡ã†ï¼š

- LangGraph Platform Concepts: Understand the foundational concepts of the LangGraph Platform.
  - LangGraph Platformã®æ¦‚å¿µï¼šLangGraph Platformã®åŸºæœ¬çš„ãªæ¦‚å¿µã‚’ç†è§£ã—ã¾ã™ã€‚
- LangGraph Platform How-to Guides: Guides for common tasks with LangGraph Platform.
  - LangGraph Platform How-toã‚¬ã‚¤ãƒ‰ï¼šLangGraph Platformã§ã®ä¸€èˆ¬çš„ãªã‚¿ã‚¹ã‚¯ã®ã‚¬ã‚¤ãƒ‰ã€‚

## Comments ã‚³ãƒ¡ãƒ³ãƒˆ

```  
